{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Keras] 利用Keras建構LSTM模型\n",
    "--以Stock Prediction 為例 1\n",
    "### Authored by PJ Wang\n",
    "### Apr 13, 2018\n",
    "#### https://medium.com/@daniel820710/%E5%88%A9%E7%94%A8keras%E5%BB%BA%E6%A7%8Blstm%E6%A8%A1%E5%9E%8B-%E4%BB%A5stock-prediction-%E7%82%BA%E4%BE%8B-1-67456e0a0b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, LSTM, TimeDistributed, RepeatVector\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load data\n",
    "## https://finance.yahoo.com/quote/SPY/history?period1=728236800&period2=1523462400&interval=1d&filter=history&frequency=1d\n",
    "## SPY dataset: Yahoo SPDR S&P 500 ETF (SPY)\n",
    "\n",
    "def readTrain():\n",
    "    dir_path = '../data' \n",
    "    file_path = dir_path + '/spy.csv'\n",
    "    train = pd.read_csv(file_path)\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Augment Features\n",
    "\n",
    "def augFeatures(train):\n",
    "  train[\"Date\"] = pd.to_datetime(train[\"Date\"])\n",
    "  train[\"year\"] = train[\"Date\"].dt.year\n",
    "  train[\"month\"] = train[\"Date\"].dt.month\n",
    "  train[\"date\"] = train[\"Date\"].dt.day\n",
    "  train[\"day\"] = train[\"Date\"].dt.dayofweek\n",
    "  return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Normalization\n",
    "\n",
    "def normalize(train):\n",
    "  train = train.drop([\"Date\"], axis=1)\n",
    "  train_norm = train.apply(lambda x: (x - np.mean(x)) / (np.max(x) - np.min(x)))\n",
    "  return train_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build Training Data\n",
    "\n",
    "def buildTrain(train, pastDay=30, futureDay=5):\n",
    "  X_train, Y_train = [], []\n",
    "  for i in range(train.shape[0]-futureDay-pastDay):\n",
    "    X_train.append(np.array(train.iloc[i:i+pastDay]))\n",
    "    Y_train.append(np.array(train.iloc[i+pastDay:i+pastDay+futureDay][\"Adj Close\"]))\n",
    "  return np.array(X_train), np.array(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## disorderize data\n",
    "\n",
    "def shuffle(X,Y):\n",
    "  np.random.seed(10)\n",
    "  randomList = np.arange(X.shape[0])\n",
    "  np.random.shuffle(randomList)\n",
    "  return X[randomList], Y[randomList]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training data & Validation data\n",
    "\n",
    "def splitData(X,Y,rate):\n",
    "  X_train = X[int(X.shape[0]*rate):]\n",
    "  Y_train = Y[int(Y.shape[0]*rate):]\n",
    "  X_val = X[:int(X.shape[0]*rate)]\n",
    "  Y_val = Y[:int(Y.shape[0]*rate)]\n",
    "  return X_train, Y_train, X_val, Y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Date      Open      High       Low    Close  Adj Close   Volume  year  \\\n",
      "0 1993-01-29  43.96875  43.96875  43.75000  43.9375  26.299288  1003200  1993   \n",
      "1 1993-02-01  43.96875  44.25000  43.96875  44.2500  26.486324   480500  1993   \n",
      "\n",
      "   month  date  day  \n",
      "0      1    29    4  \n",
      "1      2     1    0  \n",
      "        Date      Open      High       Low    Close  Adj Close   Volume  year  \\\n",
      "0 1993-01-29  43.96875  43.96875  43.75000  43.9375  26.299288  1003200  1993   \n",
      "1 1993-02-01  43.96875  44.25000  43.96875  44.2500  26.486324   480500  1993   \n",
      "\n",
      "   month  date  day  \n",
      "0      1    29    4  \n",
      "1      2     1    0  \n",
      "       Open      High       Low     Close  Adj Close    Volume      year  \\\n",
      "0 -0.342052 -0.344423 -0.340762 -0.341338  -0.301050 -0.095759 -0.486789   \n",
      "1 -0.342052 -0.343266 -0.339857 -0.340053  -0.300297 -0.096359 -0.486789   \n",
      "\n",
      "      month      date       day  \n",
      "0 -0.501146  0.442720  0.494918  \n",
      "1 -0.410237 -0.490614 -0.505082  \n"
     ]
    }
   ],
   "source": [
    "## main\n",
    "\n",
    "# read SPY.csv\n",
    "train = readTrain()\n",
    "\n",
    "# Augment the features (year, month, date, day)\n",
    "train_Aug = augFeatures(train)\n",
    "\n",
    "# Normalization\n",
    "train_norm = normalize(train_Aug)\n",
    "\n",
    "# print(train[0:2])\n",
    "# print(train_Aug[0:2])\n",
    "# print(train_norm[0:2])\n",
    "\n",
    "\n",
    "# Visualization\n",
    "# print(train_norm.size)\n",
    "# print(train_norm.shape)\n",
    "# print(train_norm[0:2])\n",
    "# print(train.iloc[0:30])\n",
    "# print(np.array(train.iloc[0:30]))\n",
    "\n",
    "# build Data, use last 30 days to predict next 5 days\n",
    "X_train, Y_train = buildTrain(train_norm, 30, 5)\n",
    "\n",
    "# shuffle the data, and random seed is 10\n",
    "X_train, Y_train = shuffle(X_train, Y_train)\n",
    "\n",
    "# split training data and validation data\n",
    "X_train, Y_train, X_val, Y_val = splitData(X_train, Y_train, 0.1)\n",
    "# X_trian: (5710, 30, 10)\n",
    "# Y_train: (5710, 5, 1)\n",
    "# X_val: (634, 30, 10)\n",
    "# Y_val: (634, 5, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "## build 1-1 model\n",
    "\n",
    "def buildOneToOneModel(shape):\n",
    "  model = Sequential()\n",
    "  model.add(LSTM(10, input_length=shape[1], input_dim=shape[2],return_sequences=True))\n",
    "  # output shape: (1, 1)\n",
    "  model.add(TimeDistributed(Dense(1)))    # or use model.add(Dense(1))\n",
    "  model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "  model.summary()\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\program\\python\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  \"\"\"\n",
      "d:\\program\\python\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(10, return_sequences=True, input_shape=(1, 10))`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 1, 10)             840       \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 1, 1)              11        \n",
      "=================================================================\n",
      "Total params: 851\n",
      "Trainable params: 851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 5710 samples, validate on 634 samples\n",
      "Epoch 1/1000\n",
      "5710/5710 [==============================] - 1s 211us/step - loss: 0.0720 - val_loss: 0.0527\n",
      "Epoch 2/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 0.0415 - val_loss: 0.0278\n",
      "Epoch 3/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 0.0190 - val_loss: 0.0102\n",
      "Epoch 4/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 0.0056 - val_loss: 0.0022\n",
      "Epoch 5/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 0.0011 - val_loss: 5.4311e-04\n",
      "Epoch 6/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 4.1119e-04 - val_loss: 3.1059e-04\n",
      "Epoch 7/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 2.8834e-04 - val_loss: 2.3919e-04\n",
      "Epoch 8/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 2.3542e-04 - val_loss: 2.0251e-04\n",
      "Epoch 9/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 2.0618e-04 - val_loss: 1.8315e-04\n",
      "Epoch 10/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 1.8858e-04 - val_loss: 1.7109e-04\n",
      "Epoch 11/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 1.7596e-04 - val_loss: 1.6174e-04\n",
      "Epoch 12/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 1.6601e-04 - val_loss: 1.5377e-04\n",
      "Epoch 13/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 1.5688e-04 - val_loss: 1.4716e-04\n",
      "Epoch 14/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 1.4872e-04 - val_loss: 1.3998e-04\n",
      "Epoch 15/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 1.4089e-04 - val_loss: 1.3369e-04\n",
      "Epoch 16/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 1.3386e-04 - val_loss: 1.2685e-04\n",
      "Epoch 17/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 1.2657e-04 - val_loss: 1.2069e-04\n",
      "Epoch 18/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 1.2051e-04 - val_loss: 1.1488e-04\n",
      "Epoch 19/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 1.1398e-04 - val_loss: 1.0966e-04\n",
      "Epoch 20/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 1.0785e-04 - val_loss: 1.0426e-04\n",
      "Epoch 21/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 1.0231e-04 - val_loss: 9.9551e-05\n",
      "Epoch 22/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 9.7268e-05 - val_loss: 9.5478e-05\n",
      "Epoch 23/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 9.2557e-05 - val_loss: 9.1078e-05\n",
      "Epoch 24/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 8.7989e-05 - val_loss: 8.6891e-05\n",
      "Epoch 25/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 8.4097e-05 - val_loss: 8.3254e-05\n",
      "Epoch 26/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 8.0268e-05 - val_loss: 8.0001e-05\n",
      "Epoch 27/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 7.6738e-05 - val_loss: 7.7022e-05\n",
      "Epoch 28/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 7.3736e-05 - val_loss: 7.4197e-05\n",
      "Epoch 29/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 7.0783e-05 - val_loss: 7.1771e-05\n",
      "Epoch 30/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 6.8376e-05 - val_loss: 6.9431e-05\n",
      "Epoch 31/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 6.5735e-05 - val_loss: 6.7383e-05\n",
      "Epoch 32/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 6.3719e-05 - val_loss: 6.5761e-05\n",
      "Epoch 33/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 6.1869e-05 - val_loss: 6.3923e-05\n",
      "Epoch 34/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 6.0257e-05 - val_loss: 6.2441e-05\n",
      "Epoch 35/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 5.8664e-05 - val_loss: 6.0910e-05\n",
      "Epoch 36/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 5.7272e-05 - val_loss: 5.9775e-05\n",
      "Epoch 37/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 5.5926e-05 - val_loss: 5.8721e-05\n",
      "Epoch 38/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 5.5024e-05 - val_loss: 5.7682e-05\n",
      "Epoch 39/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 5.3998e-05 - val_loss: 5.6984e-05\n",
      "Epoch 40/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 5.3048e-05 - val_loss: 5.6571e-05\n",
      "Epoch 41/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 5.2482e-05 - val_loss: 5.5553e-05\n",
      "Epoch 42/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 5.1583e-05 - val_loss: 5.4679e-05\n",
      "Epoch 43/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 5.0799e-05 - val_loss: 5.4490e-05\n",
      "Epoch 44/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 5.0333e-05 - val_loss: 5.3796e-05\n",
      "Epoch 45/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 4.9669e-05 - val_loss: 5.2828e-05\n",
      "Epoch 46/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 4.9059e-05 - val_loss: 5.2157e-05\n",
      "Epoch 47/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 4.8590e-05 - val_loss: 5.1728e-05\n",
      "Epoch 48/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 4.8195e-05 - val_loss: 5.1351e-05\n",
      "Epoch 49/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 4.7704e-05 - val_loss: 5.0617e-05\n",
      "Epoch 50/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 4.7154e-05 - val_loss: 5.0593e-05\n",
      "Epoch 51/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 4.6849e-05 - val_loss: 5.0096e-05\n",
      "Epoch 52/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 4.6265e-05 - val_loss: 4.9281e-05\n",
      "Epoch 53/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 4.5811e-05 - val_loss: 4.8528e-05\n",
      "Epoch 54/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 4.5467e-05 - val_loss: 4.8484e-05\n",
      "Epoch 55/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 4.5077e-05 - val_loss: 4.8090e-05\n",
      "Epoch 56/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 4.4618e-05 - val_loss: 4.7684e-05\n",
      "Epoch 57/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 4.4463e-05 - val_loss: 4.7169e-05\n",
      "Epoch 58/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 4.3909e-05 - val_loss: 4.6900e-05\n",
      "Epoch 59/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 4.3582e-05 - val_loss: 4.5909e-05\n",
      "Epoch 60/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 4.3293e-05 - val_loss: 4.5682e-05\n",
      "Epoch 61/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 4.2794e-05 - val_loss: 4.5368e-05\n",
      "Epoch 62/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 4.2874e-05 - val_loss: 4.5346e-05\n",
      "Epoch 63/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 4.2029e-05 - val_loss: 4.4408e-05\n",
      "Epoch 64/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 4.1926e-05 - val_loss: 4.4329e-05\n",
      "Epoch 65/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 4.1329e-05 - val_loss: 4.4039e-05\n",
      "Epoch 66/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 4.1244e-05 - val_loss: 4.3249e-05\n",
      "Epoch 67/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 4.0773e-05 - val_loss: 4.3024e-05\n",
      "Epoch 68/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5710/5710 [==============================] - 0s 9us/step - loss: 4.0397e-05 - val_loss: 4.3068e-05\n",
      "Epoch 69/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 4.0186e-05 - val_loss: 4.2151e-05\n",
      "Epoch 70/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 3.9901e-05 - val_loss: 4.1465e-05\n",
      "Epoch 71/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 3.9318e-05 - val_loss: 4.2272e-05\n",
      "Epoch 72/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 3.9302e-05 - val_loss: 4.0940e-05\n",
      "Epoch 73/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 3.8793e-05 - val_loss: 4.0711e-05\n",
      "Epoch 74/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 3.8155e-05 - val_loss: 4.0290e-05\n",
      "Epoch 75/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 3.7977e-05 - val_loss: 3.9750e-05\n",
      "Epoch 76/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 3.8143e-05 - val_loss: 3.9882e-05\n",
      "Epoch 77/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 3.7332e-05 - val_loss: 3.9885e-05\n",
      "Epoch 78/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 3.6905e-05 - val_loss: 3.8714e-05\n",
      "Epoch 79/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 3.6652e-05 - val_loss: 3.8316e-05\n",
      "Epoch 80/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 3.6232e-05 - val_loss: 3.7988e-05\n",
      "Epoch 81/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 3.6244e-05 - val_loss: 3.8431e-05\n",
      "Epoch 82/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 3.5662e-05 - val_loss: 3.7415e-05\n",
      "Epoch 83/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 3.5424e-05 - val_loss: 3.7470e-05\n",
      "Epoch 84/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 3.5388e-05 - val_loss: 3.6302e-05\n",
      "Epoch 85/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 3.4727e-05 - val_loss: 3.6791e-05\n",
      "Epoch 86/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 3.4542e-05 - val_loss: 3.5678e-05\n",
      "Epoch 87/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 3.4320e-05 - val_loss: 3.5819e-05\n",
      "Epoch 88/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 3.4112e-05 - val_loss: 3.5545e-05\n",
      "Epoch 89/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 3.3922e-05 - val_loss: 3.4471e-05\n",
      "Epoch 90/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 3.3245e-05 - val_loss: 3.4924e-05\n",
      "Epoch 91/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 3.3310e-05 - val_loss: 3.4760e-05\n",
      "Epoch 92/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 3.2656e-05 - val_loss: 3.4043e-05\n",
      "Epoch 93/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 3.2485e-05 - val_loss: 3.3387e-05\n",
      "Epoch 94/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 3.2715e-05 - val_loss: 3.3667e-05\n",
      "Epoch 95/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 3.1868e-05 - val_loss: 3.3594e-05\n",
      "Epoch 96/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 3.1541e-05 - val_loss: 3.2409e-05\n",
      "Epoch 97/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 3.1418e-05 - val_loss: 3.2147e-05\n",
      "Epoch 98/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 3.1014e-05 - val_loss: 3.1755e-05\n",
      "Epoch 99/1000\n",
      "5710/5710 [==============================] - 0s 7us/step - loss: 3.1074e-05 - val_loss: 3.1527e-05\n",
      "Epoch 100/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 3.0551e-05 - val_loss: 3.1295e-05\n",
      "Epoch 101/1000\n",
      "5710/5710 [==============================] - 0s 7us/step - loss: 2.9941e-05 - val_loss: 3.1069e-05\n",
      "Epoch 102/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 2.9838e-05 - val_loss: 3.0950e-05\n",
      "Epoch 103/1000\n",
      "5710/5710 [==============================] - 0s 7us/step - loss: 2.9875e-05 - val_loss: 3.0728e-05\n",
      "Epoch 104/1000\n",
      "5710/5710 [==============================] - 0s 7us/step - loss: 2.9479e-05 - val_loss: 2.9941e-05\n",
      "Epoch 105/1000\n",
      "5710/5710 [==============================] - 0s 7us/step - loss: 2.9342e-05 - val_loss: 2.9840e-05\n",
      "Epoch 106/1000\n",
      "5710/5710 [==============================] - 0s 7us/step - loss: 2.9064e-05 - val_loss: 2.9417e-05\n",
      "Epoch 107/1000\n",
      "5710/5710 [==============================] - 0s 7us/step - loss: 2.8922e-05 - val_loss: 3.0028e-05\n",
      "Epoch 108/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 2.9270e-05 - val_loss: 2.8797e-05\n",
      "Epoch 109/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 2.8588e-05 - val_loss: 2.9268e-05\n",
      "Epoch 110/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 2.8146e-05 - val_loss: 2.9077e-05\n",
      "Epoch 111/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 2.7854e-05 - val_loss: 2.8116e-05\n",
      "Epoch 112/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 2.7979e-05 - val_loss: 2.9520e-05\n",
      "Epoch 113/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 2.8569e-05 - val_loss: 2.7594e-05\n",
      "Epoch 114/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 2.7468e-05 - val_loss: 2.8260e-05\n",
      "Epoch 115/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 2.7482e-05 - val_loss: 2.7455e-05\n",
      "Epoch 116/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 2.6763e-05 - val_loss: 2.6950e-05\n",
      "Epoch 117/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 2.6623e-05 - val_loss: 2.6829e-05\n",
      "Epoch 118/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 2.6458e-05 - val_loss: 2.6266e-05\n",
      "Epoch 119/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 2.6289e-05 - val_loss: 2.6838e-05\n",
      "Epoch 120/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 2.6495e-05 - val_loss: 2.7419e-05\n",
      "Epoch 121/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 2.6420e-05 - val_loss: 2.6884e-05\n",
      "Epoch 122/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 2.6563e-05 - val_loss: 2.6885e-05\n",
      "Epoch 123/1000\n",
      "5710/5710 [==============================] - 0s 7us/step - loss: 2.6162e-05 - val_loss: 2.5602e-05\n",
      "Epoch 124/1000\n",
      "5710/5710 [==============================] - 0s 7us/step - loss: 2.5580e-05 - val_loss: 2.6010e-05\n",
      "Epoch 125/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 2.6253e-05 - val_loss: 2.5264e-05\n",
      "Epoch 126/1000\n",
      "5710/5710 [==============================] - 0s 7us/step - loss: 2.5660e-05 - val_loss: 2.6246e-05\n",
      "Epoch 127/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 2.5276e-05 - val_loss: 2.5506e-05\n",
      "Epoch 128/1000\n",
      "5710/5710 [==============================] - 0s 7us/step - loss: 2.5914e-05 - val_loss: 2.4832e-05\n",
      "Epoch 129/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 2.5095e-05 - val_loss: 2.5113e-05\n",
      "Epoch 130/1000\n",
      "5710/5710 [==============================] - 0s 7us/step - loss: 2.5301e-05 - val_loss: 2.6475e-05\n",
      "Epoch 131/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 2.5080e-05 - val_loss: 2.4800e-05\n",
      "Epoch 132/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 2.4981e-05 - val_loss: 2.4646e-05\n",
      "Epoch 133/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 2.5029e-05 - val_loss: 2.4192e-05\n",
      "Epoch 134/1000\n",
      "5710/5710 [==============================] - 0s 7us/step - loss: 2.4852e-05 - val_loss: 2.4245e-05\n",
      "Epoch 135/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 2.4734e-05 - val_loss: 2.4695e-05\n",
      "Epoch 136/1000\n",
      "5710/5710 [==============================] - 0s 7us/step - loss: 2.4679e-05 - val_loss: 2.4203e-05\n",
      "Epoch 137/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 2.5074e-05 - val_loss: 2.3693e-05\n",
      "Epoch 138/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 2.4833e-05 - val_loss: 2.4116e-05\n",
      "Epoch 139/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 2.4653e-05 - val_loss: 2.4014e-05\n",
      "Epoch 140/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5710/5710 [==============================] - 0s 8us/step - loss: 2.4606e-05 - val_loss: 2.4105e-05\n",
      "Epoch 141/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 2.4804e-05 - val_loss: 2.3937e-05\n",
      "Epoch 142/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 2.4658e-05 - val_loss: 2.4422e-05\n",
      "Epoch 143/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 2.4237e-05 - val_loss: 2.3900e-05\n",
      "Epoch 144/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 2.4050e-05 - val_loss: 2.3295e-05\n",
      "Epoch 145/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 2.4148e-05 - val_loss: 2.3424e-05\n",
      "Epoch 146/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 2.4126e-05 - val_loss: 2.3558e-05\n",
      "Epoch 147/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 2.4392e-05 - val_loss: 2.2811e-05\n",
      "Epoch 148/1000\n",
      "5710/5710 [==============================] - 0s 7us/step - loss: 2.4590e-05 - val_loss: 2.3291e-05\n",
      "Epoch 149/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 2.4002e-05 - val_loss: 2.4297e-05\n",
      "Epoch 150/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 2.4145e-05 - val_loss: 2.3547e-05\n",
      "Epoch 151/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 2.3793e-05 - val_loss: 2.3128e-05\n",
      "Epoch 152/1000\n",
      "5710/5710 [==============================] - 0s 7us/step - loss: 2.4220e-05 - val_loss: 2.2836e-05\n",
      "Epoch 153/1000\n",
      "5710/5710 [==============================] - 0s 7us/step - loss: 2.3727e-05 - val_loss: 2.3142e-05\n",
      "Epoch 154/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 2.3779e-05 - val_loss: 2.3558e-05\n",
      "Epoch 155/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 2.4206e-05 - val_loss: 2.8880e-05\n",
      "Epoch 156/1000\n",
      "5710/5710 [==============================] - 0s 7us/step - loss: 2.4699e-05 - val_loss: 2.3026e-05\n",
      "Epoch 157/1000\n",
      "5710/5710 [==============================] - 0s 7us/step - loss: 2.4108e-05 - val_loss: 2.4125e-05\n",
      "Epoch 158/1000\n",
      "5710/5710 [==============================] - 0s 7us/step - loss: 2.4287e-05 - val_loss: 2.4372e-05\n",
      "Epoch 159/1000\n",
      "5710/5710 [==============================] - 0s 7us/step - loss: 2.3843e-05 - val_loss: 2.2545e-05\n",
      "Epoch 160/1000\n",
      "5710/5710 [==============================] - 0s 7us/step - loss: 2.3838e-05 - val_loss: 2.3936e-05\n",
      "Epoch 161/1000\n",
      "5710/5710 [==============================] - 0s 7us/step - loss: 2.4126e-05 - val_loss: 2.4762e-05\n",
      "Epoch 162/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 2.4195e-05 - val_loss: 2.3160e-05\n",
      "Epoch 163/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 2.3597e-05 - val_loss: 2.2889e-05\n",
      "Epoch 164/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 2.3945e-05 - val_loss: 2.3815e-05\n",
      "Epoch 165/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 2.3877e-05 - val_loss: 2.3297e-05\n",
      "Epoch 166/1000\n",
      "5710/5710 [==============================] - 0s 7us/step - loss: 2.3513e-05 - val_loss: 2.2314e-05\n",
      "Epoch 167/1000\n",
      "5710/5710 [==============================] - 0s 7us/step - loss: 2.3939e-05 - val_loss: 2.2210e-05\n",
      "Epoch 168/1000\n",
      "5710/5710 [==============================] - 0s 7us/step - loss: 2.3602e-05 - val_loss: 2.4090e-05\n",
      "Epoch 169/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 2.4220e-05 - val_loss: 2.7477e-05\n",
      "Epoch 170/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 2.4572e-05 - val_loss: 2.4034e-05\n",
      "Epoch 171/1000\n",
      "5710/5710 [==============================] - 0s 7us/step - loss: 2.4133e-05 - val_loss: 2.2520e-05\n",
      "Epoch 172/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 2.3308e-05 - val_loss: 2.2186e-05\n",
      "Epoch 173/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 2.4617e-05 - val_loss: 2.3144e-05\n",
      "Epoch 174/1000\n",
      "5710/5710 [==============================] - 0s 7us/step - loss: 2.3716e-05 - val_loss: 2.2474e-05\n",
      "Epoch 175/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 2.3691e-05 - val_loss: 2.7433e-05\n",
      "Epoch 176/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 2.4385e-05 - val_loss: 2.2823e-05\n",
      "Epoch 177/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 2.3911e-05 - val_loss: 2.2630e-05\n",
      "Epoch 178/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 2.3337e-05 - val_loss: 2.3735e-05\n",
      "Epoch 179/1000\n",
      "5710/5710 [==============================] - 0s 7us/step - loss: 2.4151e-05 - val_loss: 2.2733e-05\n",
      "Epoch 180/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 2.3644e-05 - val_loss: 2.3159e-05\n",
      "Epoch 181/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 2.3716e-05 - val_loss: 2.3056e-05\n",
      "Epoch 182/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 2.4075e-05 - val_loss: 2.2175e-05\n",
      "Epoch 00182: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x24359a7ef48>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## run 1-1 model\n",
    "\n",
    "train = readTrain()\n",
    "train_Aug = augFeatures(train)\n",
    "train_norm = normalize(train_Aug)\n",
    "# change the last day and next day \n",
    "X_train, Y_train = buildTrain(train_norm, 1, 1)\n",
    "X_train, Y_train = shuffle(X_train, Y_train)\n",
    "X_train, Y_train, X_val, Y_val = splitData(X_train, Y_train, 0.1)\n",
    "\n",
    "# from 2 dimmension to 3 dimension\n",
    "Y_train = Y_train[:,np.newaxis]\n",
    "Y_val = Y_val[:,np.newaxis]\n",
    "\n",
    "model = buildOneToOneModel(X_train.shape)\n",
    "callback = EarlyStopping(monitor=\"loss\", patience=10, verbose=1, mode=\"auto\")\n",
    "model.fit(X_train, Y_train, epochs=1000, batch_size=128, validation_data=(X_val, Y_val), callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "## build n-1 model\n",
    "\n",
    "def buildManyToOneModel(shape):\n",
    "  model = Sequential()\n",
    "  model.add(LSTM(10, input_length=shape[1], input_dim=shape[2]))\n",
    "  # output shape: (1, 1)\n",
    "  model.add(Dense(1))\n",
    "  model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "  model.summary()\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\program\\python\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  \"\"\"\n",
      "d:\\program\\python\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(10, input_shape=(30, 10))`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (None, 10)                840       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 851\n",
      "Trainable params: 851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 5684 samples, validate on 631 samples\n",
      "Epoch 1/1000\n",
      "5684/5684 [==============================] - 1s 168us/step - loss: 0.0050 - val_loss: 0.0013\n",
      "Epoch 2/1000\n",
      "5684/5684 [==============================] - 0s 57us/step - loss: 8.8673e-04 - val_loss: 4.7710e-04\n",
      "Epoch 3/1000\n",
      "5684/5684 [==============================] - 0s 54us/step - loss: 4.0300e-04 - val_loss: 2.5999e-04\n",
      "Epoch 4/1000\n",
      "5684/5684 [==============================] - 0s 55us/step - loss: 2.3340e-04 - val_loss: 1.6843e-04\n",
      "Epoch 5/1000\n",
      "5684/5684 [==============================] - 0s 54us/step - loss: 1.5791e-04 - val_loss: 1.2911e-04\n",
      "Epoch 6/1000\n",
      "5684/5684 [==============================] - 0s 57us/step - loss: 1.1854e-04 - val_loss: 1.0732e-04\n",
      "Epoch 7/1000\n",
      "5684/5684 [==============================] - 0s 56us/step - loss: 9.6323e-05 - val_loss: 9.1530e-05\n",
      "Epoch 8/1000\n",
      "5684/5684 [==============================] - 0s 56us/step - loss: 8.2304e-05 - val_loss: 8.1574e-05\n",
      "Epoch 9/1000\n",
      "5684/5684 [==============================] - 0s 56us/step - loss: 7.1635e-05 - val_loss: 7.9663e-05\n",
      "Epoch 10/1000\n",
      "5684/5684 [==============================] - 0s 57us/step - loss: 6.5142e-05 - val_loss: 7.2427e-05\n",
      "Epoch 11/1000\n",
      "5684/5684 [==============================] - 0s 56us/step - loss: 5.9636e-05 - val_loss: 6.5643e-05\n",
      "Epoch 12/1000\n",
      "5684/5684 [==============================] - 0s 56us/step - loss: 5.4999e-05 - val_loss: 6.1903e-05\n",
      "Epoch 13/1000\n",
      "5684/5684 [==============================] - 0s 56us/step - loss: 5.2076e-05 - val_loss: 6.0494e-05\n",
      "Epoch 14/1000\n",
      "5684/5684 [==============================] - 0s 57us/step - loss: 4.9941e-05 - val_loss: 5.8803e-05\n",
      "Epoch 15/1000\n",
      "5684/5684 [==============================] - 0s 56us/step - loss: 4.8101e-05 - val_loss: 5.6739e-05\n",
      "Epoch 16/1000\n",
      "5684/5684 [==============================] - 0s 58us/step - loss: 4.6338e-05 - val_loss: 5.4699e-05\n",
      "Epoch 17/1000\n",
      "5684/5684 [==============================] - 0s 57us/step - loss: 4.4994e-05 - val_loss: 5.5666e-05\n",
      "Epoch 18/1000\n",
      "5684/5684 [==============================] - 0s 56us/step - loss: 4.3581e-05 - val_loss: 5.4344e-05\n",
      "Epoch 19/1000\n",
      "5684/5684 [==============================] - 0s 56us/step - loss: 4.2582e-05 - val_loss: 5.1245e-05\n",
      "Epoch 20/1000\n",
      "5684/5684 [==============================] - 0s 56us/step - loss: 4.1513e-05 - val_loss: 5.0385e-05\n",
      "Epoch 21/1000\n",
      "5684/5684 [==============================] - 0s 57us/step - loss: 4.0995e-05 - val_loss: 4.9826e-05\n",
      "Epoch 22/1000\n",
      "5684/5684 [==============================] - 0s 56us/step - loss: 3.9723e-05 - val_loss: 5.2255e-05\n",
      "Epoch 23/1000\n",
      "5684/5684 [==============================] - 0s 56us/step - loss: 3.9718e-05 - val_loss: 4.8556e-05\n",
      "Epoch 24/1000\n",
      "5684/5684 [==============================] - 0s 58us/step - loss: 3.8527e-05 - val_loss: 4.8529e-05\n",
      "Epoch 25/1000\n",
      "5684/5684 [==============================] - 0s 57us/step - loss: 3.8958e-05 - val_loss: 4.9079e-05\n",
      "Epoch 26/1000\n",
      "5684/5684 [==============================] - 0s 70us/step - loss: 3.8549e-05 - val_loss: 4.8303e-05\n",
      "Epoch 27/1000\n",
      "5684/5684 [==============================] - 0s 80us/step - loss: 3.7922e-05 - val_loss: 4.7278e-05\n",
      "Epoch 28/1000\n",
      "5684/5684 [==============================] - 0s 82us/step - loss: 3.6594e-05 - val_loss: 4.5407e-05\n",
      "Epoch 29/1000\n",
      "5684/5684 [==============================] - 0s 83us/step - loss: 3.6294e-05 - val_loss: 4.5546e-05\n",
      "Epoch 30/1000\n",
      "5684/5684 [==============================] - 0s 81us/step - loss: 3.6120e-05 - val_loss: 4.4968e-05\n",
      "Epoch 31/1000\n",
      "5684/5684 [==============================] - 0s 83us/step - loss: 3.5639e-05 - val_loss: 4.3810e-05\n",
      "Epoch 32/1000\n",
      "5684/5684 [==============================] - 0s 80us/step - loss: 3.5237e-05 - val_loss: 4.5034e-05\n",
      "Epoch 33/1000\n",
      "5684/5684 [==============================] - 0s 82us/step - loss: 3.5792e-05 - val_loss: 4.3459e-05\n",
      "Epoch 34/1000\n",
      "5684/5684 [==============================] - 0s 85us/step - loss: 3.5149e-05 - val_loss: 4.3345e-05\n",
      "Epoch 35/1000\n",
      "5684/5684 [==============================] - 0s 84us/step - loss: 3.4055e-05 - val_loss: 4.3636e-05\n",
      "Epoch 36/1000\n",
      "5684/5684 [==============================] - 0s 81us/step - loss: 3.4289e-05 - val_loss: 4.4153e-05\n",
      "Epoch 37/1000\n",
      "5684/5684 [==============================] - 0s 80us/step - loss: 3.4072e-05 - val_loss: 4.1347e-05\n",
      "Epoch 38/1000\n",
      "5684/5684 [==============================] - 0s 80us/step - loss: 3.3735e-05 - val_loss: 4.4634e-05\n",
      "Epoch 39/1000\n",
      "5684/5684 [==============================] - 0s 78us/step - loss: 3.3408e-05 - val_loss: 4.2541e-05\n",
      "Epoch 40/1000\n",
      "5684/5684 [==============================] - 0s 79us/step - loss: 3.3022e-05 - val_loss: 4.1244e-05\n",
      "Epoch 41/1000\n",
      "5684/5684 [==============================] - 0s 79us/step - loss: 3.1987e-05 - val_loss: 4.0625e-05\n",
      "Epoch 42/1000\n",
      "5684/5684 [==============================] - 0s 79us/step - loss: 3.3353e-05 - val_loss: 3.9192e-05\n",
      "Epoch 43/1000\n",
      "5684/5684 [==============================] - 0s 79us/step - loss: 3.2491e-05 - val_loss: 3.9998e-05\n",
      "Epoch 44/1000\n",
      "5684/5684 [==============================] - 0s 79us/step - loss: 3.2046e-05 - val_loss: 3.9769e-05\n",
      "Epoch 45/1000\n",
      "5684/5684 [==============================] - 0s 79us/step - loss: 3.1326e-05 - val_loss: 3.9251e-05\n",
      "Epoch 46/1000\n",
      "5684/5684 [==============================] - 0s 79us/step - loss: 3.1779e-05 - val_loss: 3.8131e-05\n",
      "Epoch 47/1000\n",
      "5684/5684 [==============================] - 0s 80us/step - loss: 3.1948e-05 - val_loss: 3.9954e-05\n",
      "Epoch 48/1000\n",
      "5684/5684 [==============================] - 0s 80us/step - loss: 3.1483e-05 - val_loss: 3.9185e-05\n",
      "Epoch 49/1000\n",
      "5684/5684 [==============================] - 0s 78us/step - loss: 3.1392e-05 - val_loss: 3.7505e-05\n",
      "Epoch 50/1000\n",
      "5684/5684 [==============================] - 0s 79us/step - loss: 3.1534e-05 - val_loss: 3.7570e-05\n",
      "Epoch 51/1000\n",
      "5684/5684 [==============================] - 0s 80us/step - loss: 3.1206e-05 - val_loss: 3.6539e-05\n",
      "Epoch 52/1000\n",
      "5684/5684 [==============================] - 0s 79us/step - loss: 3.0884e-05 - val_loss: 3.8653e-05\n",
      "Epoch 53/1000\n",
      "5684/5684 [==============================] - 0s 80us/step - loss: 3.0225e-05 - val_loss: 3.6255e-05\n",
      "Epoch 54/1000\n",
      "5684/5684 [==============================] - 0s 78us/step - loss: 3.0279e-05 - val_loss: 3.7223e-05\n",
      "Epoch 55/1000\n",
      "5684/5684 [==============================] - 0s 78us/step - loss: 2.9890e-05 - val_loss: 3.7982e-05\n",
      "Epoch 56/1000\n",
      "5684/5684 [==============================] - 0s 77us/step - loss: 3.0567e-05 - val_loss: 3.5643e-05\n",
      "Epoch 57/1000\n",
      "5684/5684 [==============================] - 0s 79us/step - loss: 2.9949e-05 - val_loss: 3.6513e-05\n",
      "Epoch 58/1000\n",
      "5684/5684 [==============================] - 0s 80us/step - loss: 2.9660e-05 - val_loss: 3.4496e-05\n",
      "Epoch 59/1000\n",
      "5684/5684 [==============================] - 0s 79us/step - loss: 3.2341e-05 - val_loss: 3.4175e-05\n",
      "Epoch 60/1000\n",
      "5684/5684 [==============================] - 0s 78us/step - loss: 2.9313e-05 - val_loss: 3.4731e-05\n",
      "Epoch 61/1000\n",
      "5684/5684 [==============================] - 0s 79us/step - loss: 2.9560e-05 - val_loss: 3.5869e-05\n",
      "Epoch 62/1000\n",
      "5684/5684 [==============================] - 0s 80us/step - loss: 2.9539e-05 - val_loss: 3.5826e-05\n",
      "Epoch 63/1000\n",
      "5684/5684 [==============================] - 0s 78us/step - loss: 2.9238e-05 - val_loss: 3.7511e-05\n",
      "Epoch 64/1000\n",
      "5684/5684 [==============================] - 0s 80us/step - loss: 2.9123e-05 - val_loss: 4.0157e-05\n",
      "Epoch 65/1000\n",
      "5684/5684 [==============================] - 0s 80us/step - loss: 2.9238e-05 - val_loss: 3.3477e-05\n",
      "Epoch 66/1000\n",
      "5684/5684 [==============================] - 0s 78us/step - loss: 2.8500e-05 - val_loss: 3.6760e-05\n",
      "Epoch 67/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5684/5684 [==============================] - 0s 77us/step - loss: 2.9613e-05 - val_loss: 3.4229e-05\n",
      "Epoch 68/1000\n",
      "5684/5684 [==============================] - 0s 78us/step - loss: 2.8661e-05 - val_loss: 4.0018e-05\n",
      "Epoch 69/1000\n",
      "5684/5684 [==============================] - 0s 77us/step - loss: 2.9721e-05 - val_loss: 3.3491e-05\n",
      "Epoch 70/1000\n",
      "5684/5684 [==============================] - 0s 78us/step - loss: 2.8687e-05 - val_loss: 3.4081e-05\n",
      "Epoch 71/1000\n",
      "5684/5684 [==============================] - 0s 76us/step - loss: 2.8242e-05 - val_loss: 3.2888e-05\n",
      "Epoch 72/1000\n",
      "5684/5684 [==============================] - 0s 77us/step - loss: 2.7980e-05 - val_loss: 3.2435e-05\n",
      "Epoch 73/1000\n",
      "5684/5684 [==============================] - 0s 77us/step - loss: 2.8782e-05 - val_loss: 3.3198e-05\n",
      "Epoch 74/1000\n",
      "5684/5684 [==============================] - 0s 78us/step - loss: 2.8100e-05 - val_loss: 3.1539e-05\n",
      "Epoch 75/1000\n",
      "5684/5684 [==============================] - 0s 79us/step - loss: 2.8196e-05 - val_loss: 3.4918e-05\n",
      "Epoch 76/1000\n",
      "5684/5684 [==============================] - 0s 81us/step - loss: 2.8512e-05 - val_loss: 3.2865e-05\n",
      "Epoch 77/1000\n",
      "5684/5684 [==============================] - 0s 79us/step - loss: 2.7488e-05 - val_loss: 3.2981e-05\n",
      "Epoch 78/1000\n",
      "5684/5684 [==============================] - 0s 77us/step - loss: 2.6848e-05 - val_loss: 3.1442e-05\n",
      "Epoch 79/1000\n",
      "5684/5684 [==============================] - 0s 79us/step - loss: 2.8022e-05 - val_loss: 3.3982e-05\n",
      "Epoch 80/1000\n",
      "5684/5684 [==============================] - 0s 76us/step - loss: 2.7642e-05 - val_loss: 3.2197e-05\n",
      "Epoch 81/1000\n",
      "5684/5684 [==============================] - 0s 78us/step - loss: 2.9399e-05 - val_loss: 3.2250e-05\n",
      "Epoch 82/1000\n",
      "5684/5684 [==============================] - 0s 78us/step - loss: 2.9057e-05 - val_loss: 3.8621e-05\n",
      "Epoch 83/1000\n",
      "5684/5684 [==============================] - 0s 79us/step - loss: 2.8238e-05 - val_loss: 3.1060e-05\n",
      "Epoch 84/1000\n",
      "5684/5684 [==============================] - 0s 80us/step - loss: 2.6691e-05 - val_loss: 3.3813e-05\n",
      "Epoch 85/1000\n",
      "5684/5684 [==============================] - 0s 83us/step - loss: 2.7197e-05 - val_loss: 3.5909e-05\n",
      "Epoch 86/1000\n",
      "5684/5684 [==============================] - 0s 79us/step - loss: 2.7565e-05 - val_loss: 3.4885e-05\n",
      "Epoch 87/1000\n",
      "5684/5684 [==============================] - 0s 77us/step - loss: 2.7686e-05 - val_loss: 3.1418e-05\n",
      "Epoch 88/1000\n",
      "5684/5684 [==============================] - 0s 76us/step - loss: 2.7189e-05 - val_loss: 3.4165e-05\n",
      "Epoch 89/1000\n",
      "5684/5684 [==============================] - 0s 80us/step - loss: 2.6847e-05 - val_loss: 3.0697e-05\n",
      "Epoch 90/1000\n",
      "5684/5684 [==============================] - 0s 77us/step - loss: 2.7027e-05 - val_loss: 3.1486e-05\n",
      "Epoch 91/1000\n",
      "5684/5684 [==============================] - 0s 83us/step - loss: 2.7996e-05 - val_loss: 3.1862e-05\n",
      "Epoch 92/1000\n",
      "5684/5684 [==============================] - 0s 78us/step - loss: 2.7319e-05 - val_loss: 3.0982e-05\n",
      "Epoch 93/1000\n",
      "5684/5684 [==============================] - 0s 76us/step - loss: 2.6793e-05 - val_loss: 3.1172e-05\n",
      "Epoch 94/1000\n",
      "5684/5684 [==============================] - 0s 76us/step - loss: 2.6779e-05 - val_loss: 2.9834e-05\n",
      "Epoch 00094: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x2435e444408>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## run n-1 model\n",
    "\n",
    "train = readTrain()\n",
    "train_Aug = augFeatures(train)\n",
    "train_norm = normalize(train_Aug)\n",
    "# change the last day and next day \n",
    "X_train, Y_train = buildTrain(train_norm, 30, 1)\n",
    "X_train, Y_train = shuffle(X_train, Y_train)\n",
    "# because no return sequence, Y_train and Y_val shape must be 2 dimension\n",
    "X_train, Y_train, X_val, Y_val = splitData(X_train, Y_train, 0.1)\n",
    "\n",
    "model = buildManyToOneModel(X_train.shape)\n",
    "callback = EarlyStopping(monitor=\"loss\", patience=10, verbose=1, mode=\"auto\")\n",
    "model.fit(X_train, Y_train, epochs=1000, batch_size=128, validation_data=(X_val, Y_val), callbacks=[callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "## build 1-n model\n",
    "\n",
    "def buildOneToManyModel(shape):\n",
    "  model = Sequential()\n",
    "  model.add(LSTM(10, input_length=shape[1], input_dim=shape[2]))\n",
    "  # output shape: (5, 1)\n",
    "  model.add(Dense(1))\n",
    "  model.add(RepeatVector(5))\n",
    "  model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "  model.summary()\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\program\\python\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  \"\"\"\n",
      "d:\\program\\python\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(10, input_shape=(1, 10))`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 10)                840       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 11        \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 5, 1)              0         \n",
      "=================================================================\n",
      "Total params: 851\n",
      "Trainable params: 851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 5706 samples, validate on 634 samples\n",
      "Epoch 1/1000\n",
      "5706/5706 [==============================] - 1s 132us/step - loss: 0.0102 - val_loss: 0.0028\n",
      "Epoch 2/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 9.3778e-04 - val_loss: 2.9156e-04\n",
      "Epoch 3/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 2.7569e-04 - val_loss: 2.4244e-04\n",
      "Epoch 4/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 2.3495e-04 - val_loss: 2.1379e-04\n",
      "Epoch 5/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 2.0617e-04 - val_loss: 1.8937e-04\n",
      "Epoch 6/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 1.8346e-04 - val_loss: 1.6873e-04\n",
      "Epoch 7/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 1.6295e-04 - val_loss: 1.5102e-04\n",
      "Epoch 8/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 1.4625e-04 - val_loss: 1.3616e-04\n",
      "Epoch 9/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 1.3123e-04 - val_loss: 1.2107e-04\n",
      "Epoch 10/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 1.1929e-04 - val_loss: 1.1301e-04\n",
      "Epoch 11/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 1.0967e-04 - val_loss: 1.0179e-04\n",
      "Epoch 12/1000\n",
      "5706/5706 [==============================] - 0s 9us/step - loss: 1.0158e-04 - val_loss: 9.4832e-05\n",
      "Epoch 13/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 9.4937e-05 - val_loss: 8.9147e-05\n",
      "Epoch 14/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 8.9534e-05 - val_loss: 8.4911e-05\n",
      "Epoch 15/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 8.5553e-05 - val_loss: 8.0752e-05\n",
      "Epoch 16/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 8.2816e-05 - val_loss: 7.8294e-05\n",
      "Epoch 17/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 7.9852e-05 - val_loss: 7.6985e-05\n",
      "Epoch 18/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 7.7956e-05 - val_loss: 7.4088e-05\n",
      "Epoch 19/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 7.6205e-05 - val_loss: 7.2698e-05\n",
      "Epoch 20/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 7.5045e-05 - val_loss: 7.1638e-05\n",
      "Epoch 21/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 7.4102e-05 - val_loss: 7.5065e-05\n",
      "Epoch 22/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 7.4015e-05 - val_loss: 7.0098e-05\n",
      "Epoch 23/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 7.2779e-05 - val_loss: 7.0683e-05\n",
      "Epoch 24/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 7.2006e-05 - val_loss: 7.0032e-05\n",
      "Epoch 25/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 7.1340e-05 - val_loss: 6.9257e-05\n",
      "Epoch 26/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 7.1116e-05 - val_loss: 6.8876e-05\n",
      "Epoch 27/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 7.0810e-05 - val_loss: 6.8618e-05\n",
      "Epoch 28/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 7.0393e-05 - val_loss: 6.8737e-05\n",
      "Epoch 29/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 7.0250e-05 - val_loss: 6.7588e-05\n",
      "Epoch 30/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 6.9898e-05 - val_loss: 6.7823e-05\n",
      "Epoch 31/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 6.9665e-05 - val_loss: 6.7756e-05\n",
      "Epoch 32/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 6.9216e-05 - val_loss: 6.6850e-05\n",
      "Epoch 33/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 6.8645e-05 - val_loss: 6.7057e-05\n",
      "Epoch 34/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 6.8771e-05 - val_loss: 6.6349e-05\n",
      "Epoch 35/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 6.8660e-05 - val_loss: 6.7250e-05\n",
      "Epoch 36/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 6.7900e-05 - val_loss: 6.6659e-05\n",
      "Epoch 37/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 6.8338e-05 - val_loss: 6.6607e-05\n",
      "Epoch 38/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 6.7154e-05 - val_loss: 6.5416e-05\n",
      "Epoch 39/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 6.7195e-05 - val_loss: 6.4973e-05\n",
      "Epoch 40/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 6.7494e-05 - val_loss: 6.5241e-05\n",
      "Epoch 41/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 6.6653e-05 - val_loss: 6.5002e-05\n",
      "Epoch 42/1000\n",
      "5706/5706 [==============================] - 0s 9us/step - loss: 6.6250e-05 - val_loss: 6.4593e-05\n",
      "Epoch 43/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 6.6253e-05 - val_loss: 6.3761e-05\n",
      "Epoch 44/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 6.5893e-05 - val_loss: 6.3776e-05\n",
      "Epoch 45/1000\n",
      "5706/5706 [==============================] - 0s 9us/step - loss: 6.5970e-05 - val_loss: 6.5152e-05\n",
      "Epoch 46/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 6.6233e-05 - val_loss: 6.2870e-05\n",
      "Epoch 47/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 6.4760e-05 - val_loss: 6.2429e-05\n",
      "Epoch 48/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 6.5146e-05 - val_loss: 6.2973e-05\n",
      "Epoch 49/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 6.4449e-05 - val_loss: 6.3007e-05\n",
      "Epoch 50/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 6.4225e-05 - val_loss: 6.2073e-05\n",
      "Epoch 51/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 6.3808e-05 - val_loss: 6.1795e-05\n",
      "Epoch 52/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 6.3854e-05 - val_loss: 6.1289e-05\n",
      "Epoch 53/1000\n",
      "5706/5706 [==============================] - 0s 9us/step - loss: 6.3239e-05 - val_loss: 6.1826e-05\n",
      "Epoch 54/1000\n",
      "5706/5706 [==============================] - 0s 8us/step - loss: 6.3469e-05 - val_loss: 6.1451e-05\n",
      "Epoch 55/1000\n",
      "5706/5706 [==============================] - 0s 8us/step - loss: 6.3037e-05 - val_loss: 6.2038e-05\n",
      "Epoch 56/1000\n",
      "5706/5706 [==============================] - 0s 9us/step - loss: 6.3178e-05 - val_loss: 6.0827e-05\n",
      "Epoch 57/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 6.3696e-05 - val_loss: 6.3369e-05\n",
      "Epoch 58/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 6.2360e-05 - val_loss: 5.9472e-05\n",
      "Epoch 59/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 6.2056e-05 - val_loss: 6.0076e-05\n",
      "Epoch 60/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 6.1833e-05 - val_loss: 5.9501e-05\n",
      "Epoch 61/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 6.1248e-05 - val_loss: 5.9340e-05\n",
      "Epoch 62/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 6.1705e-05 - val_loss: 6.0909e-05\n",
      "Epoch 63/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 6.2592e-05 - val_loss: 5.9180e-05\n",
      "Epoch 64/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 6.0606e-05 - val_loss: 5.8496e-05\n",
      "Epoch 65/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 6.1006e-05 - val_loss: 6.0643e-05\n",
      "Epoch 66/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5706/5706 [==============================] - 0s 12us/step - loss: 6.0635e-05 - val_loss: 5.7510e-05\n",
      "Epoch 67/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 6.0637e-05 - val_loss: 5.7646e-05\n",
      "Epoch 68/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 6.0439e-05 - val_loss: 5.8870e-05\n",
      "Epoch 69/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 5.9618e-05 - val_loss: 5.7251e-05\n",
      "Epoch 70/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 6.0230e-05 - val_loss: 5.8343e-05\n",
      "Epoch 71/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 6.0246e-05 - val_loss: 5.7281e-05\n",
      "Epoch 72/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.9448e-05 - val_loss: 5.8081e-05\n",
      "Epoch 73/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.9592e-05 - val_loss: 5.6669e-05\n",
      "Epoch 74/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.9345e-05 - val_loss: 5.6688e-05\n",
      "Epoch 75/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 5.9257e-05 - val_loss: 5.6220e-05\n",
      "Epoch 76/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 5.8929e-05 - val_loss: 5.5845e-05\n",
      "Epoch 77/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.9424e-05 - val_loss: 5.6591e-05\n",
      "Epoch 78/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 5.8634e-05 - val_loss: 5.5575e-05\n",
      "Epoch 79/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.8387e-05 - val_loss: 5.6437e-05\n",
      "Epoch 80/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 5.8576e-05 - val_loss: 5.6386e-05\n",
      "Epoch 81/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 5.8256e-05 - val_loss: 5.6035e-05\n",
      "Epoch 82/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 5.9282e-05 - val_loss: 5.6762e-05\n",
      "Epoch 83/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 5.8179e-05 - val_loss: 5.6705e-05\n",
      "Epoch 84/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.8451e-05 - val_loss: 5.6151e-05\n",
      "Epoch 85/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 5.8740e-05 - val_loss: 5.5381e-05\n",
      "Epoch 86/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 5.7732e-05 - val_loss: 5.7764e-05\n",
      "Epoch 87/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.8096e-05 - val_loss: 5.4572e-05\n",
      "Epoch 88/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.7950e-05 - val_loss: 5.4507e-05\n",
      "Epoch 89/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 5.8073e-05 - val_loss: 5.4596e-05\n",
      "Epoch 90/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 5.7380e-05 - val_loss: 5.5490e-05\n",
      "Epoch 91/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 5.7817e-05 - val_loss: 5.4366e-05\n",
      "Epoch 92/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 5.7564e-05 - val_loss: 5.4109e-05\n",
      "Epoch 93/1000\n",
      "5706/5706 [==============================] - ETA: 0s - loss: 5.6978e-0 - 0s 11us/step - loss: 5.7817e-05 - val_loss: 5.5274e-05\n",
      "Epoch 94/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.7658e-05 - val_loss: 5.4432e-05\n",
      "Epoch 95/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.7902e-05 - val_loss: 5.4253e-05\n",
      "Epoch 96/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 5.7311e-05 - val_loss: 5.4001e-05\n",
      "Epoch 97/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.7684e-05 - val_loss: 5.4695e-05\n",
      "Epoch 98/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.7316e-05 - val_loss: 5.4290e-05\n",
      "Epoch 99/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.7572e-05 - val_loss: 5.8066e-05\n",
      "Epoch 100/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 5.7685e-05 - val_loss: 5.5893e-05\n",
      "Epoch 101/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.7178e-05 - val_loss: 5.4341e-05\n",
      "Epoch 102/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.6971e-05 - val_loss: 5.6805e-05\n",
      "Epoch 103/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 5.7730e-05 - val_loss: 5.5998e-05\n",
      "Epoch 104/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 5.7668e-05 - val_loss: 5.4727e-05\n",
      "Epoch 105/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 5.6725e-05 - val_loss: 5.4022e-05\n",
      "Epoch 106/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 5.6936e-05 - val_loss: 5.6408e-05\n",
      "Epoch 107/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 5.7244e-05 - val_loss: 5.5645e-05\n",
      "Epoch 108/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.8161e-05 - val_loss: 5.4663e-05\n",
      "Epoch 109/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.6656e-05 - val_loss: 5.3811e-05\n",
      "Epoch 110/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 5.7518e-05 - val_loss: 5.5117e-05\n",
      "Epoch 111/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 5.7038e-05 - val_loss: 5.6181e-05\n",
      "Epoch 112/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 5.7575e-05 - val_loss: 5.4867e-05\n",
      "Epoch 113/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 5.6747e-05 - val_loss: 5.3792e-05\n",
      "Epoch 114/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 5.6641e-05 - val_loss: 5.4146e-05\n",
      "Epoch 115/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 5.8219e-05 - val_loss: 5.5923e-05\n",
      "Epoch 116/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 5.7460e-05 - val_loss: 5.4159e-05\n",
      "Epoch 117/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 5.6188e-05 - val_loss: 5.3962e-05\n",
      "Epoch 118/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.6850e-05 - val_loss: 5.3779e-05\n",
      "Epoch 119/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.6836e-05 - val_loss: 5.3295e-05\n",
      "Epoch 120/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 5.6691e-05 - val_loss: 5.6616e-05\n",
      "Epoch 121/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.7790e-05 - val_loss: 5.5002e-05\n",
      "Epoch 122/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 5.6712e-05 - val_loss: 5.5792e-05\n",
      "Epoch 123/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 5.6463e-05 - val_loss: 5.3985e-05\n",
      "Epoch 124/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 5.6782e-05 - val_loss: 5.7002e-05\n",
      "Epoch 125/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 5.6570e-05 - val_loss: 5.4322e-05\n",
      "Epoch 126/1000\n",
      "5706/5706 [==============================] - ETA: 0s - loss: 5.6136e-0 - 0s 12us/step - loss: 5.6741e-05 - val_loss: 5.3932e-05\n",
      "Epoch 127/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 5.7757e-05 - val_loss: 5.4338e-05\n",
      "Epoch 00127: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x2435e5fcec8>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## run 1-n model \n",
    "\n",
    "train = readTrain()\n",
    "train_Aug = augFeatures(train)\n",
    "train_norm = normalize(train_Aug)\n",
    "# change the last day and next day \n",
    "X_train, Y_train = buildTrain(train_norm, 1, 5)\n",
    "X_train, Y_train = shuffle(X_train, Y_train)\n",
    "X_train, Y_train, X_val, Y_val = splitData(X_train, Y_train, 0.1)\n",
    "\n",
    "# from 2 dimmension to 3 dimension\n",
    "Y_train = Y_train[:,:,np.newaxis]\n",
    "Y_val = Y_val[:,:,np.newaxis]\n",
    "\n",
    "model = buildOneToManyModel(X_train.shape)\n",
    "callback = EarlyStopping(monitor=\"loss\", patience=10, verbose=1, mode=\"auto\")\n",
    "model.fit(X_train, Y_train, epochs=1000, batch_size=128, validation_data=(X_val, Y_val), callbacks=[callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "## build n-n model\n",
    "\n",
    "def buildManyToManyModel(shape):\n",
    "  model = Sequential()\n",
    "  model.add(LSTM(10, input_length=shape[1], input_dim=shape[2], return_sequences=True))\n",
    "  # output shape: (5, 1)\n",
    "  model.add(TimeDistributed(Dense(1)))\n",
    "  model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "  model.summary()\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\program\\python\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  \"\"\"\n",
      "d:\\program\\python\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(10, return_sequences=True, input_shape=(5, 10))`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_6 (LSTM)                (None, 5, 10)             840       \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 5, 1)              11        \n",
      "=================================================================\n",
      "Total params: 851\n",
      "Trainable params: 851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 5703 samples, validate on 633 samples\n",
      "Epoch 1/1000\n",
      "5703/5703 [==============================] - 0s 83us/step - loss: 0.0422 - val_loss: 0.0181\n",
      "Epoch 2/1000\n",
      "5703/5703 [==============================] - 0s 35us/step - loss: 0.0111 - val_loss: 0.0062\n",
      "Epoch 3/1000\n",
      "5703/5703 [==============================] - 0s 28us/step - loss: 0.0060 - val_loss: 0.0048\n",
      "Epoch 4/1000\n",
      "5703/5703 [==============================] - 0s 36us/step - loss: 0.0046 - val_loss: 0.0037\n",
      "Epoch 5/1000\n",
      "5703/5703 [==============================] - 0s 35us/step - loss: 0.0036 - val_loss: 0.0029\n",
      "Epoch 6/1000\n",
      "5703/5703 [==============================] - 0s 31us/step - loss: 0.0027 - val_loss: 0.0022\n",
      "Epoch 7/1000\n",
      "5703/5703 [==============================] - 0s 35us/step - loss: 0.0020 - val_loss: 0.0016\n",
      "Epoch 8/1000\n",
      "5703/5703 [==============================] - 0s 32us/step - loss: 0.0015 - val_loss: 0.0012\n",
      "Epoch 9/1000\n",
      "5703/5703 [==============================] - 0s 34us/step - loss: 0.0011 - val_loss: 9.5536e-04\n",
      "Epoch 10/1000\n",
      "5703/5703 [==============================] - 0s 36us/step - loss: 8.9110e-04 - val_loss: 7.6925e-04\n",
      "Epoch 11/1000\n",
      "5703/5703 [==============================] - 0s 37us/step - loss: 7.1862e-04 - val_loss: 6.3252e-04\n",
      "Epoch 12/1000\n",
      "5703/5703 [==============================] - 0s 35us/step - loss: 5.8989e-04 - val_loss: 5.2961e-04\n",
      "Epoch 13/1000\n",
      "5703/5703 [==============================] - 0s 34us/step - loss: 4.9048e-04 - val_loss: 4.5011e-04\n",
      "Epoch 14/1000\n",
      "5703/5703 [==============================] - 0s 35us/step - loss: 4.1145e-04 - val_loss: 3.8787e-04\n",
      "Epoch 15/1000\n",
      "5703/5703 [==============================] - 0s 34us/step - loss: 3.5092e-04 - val_loss: 3.3681e-04\n",
      "Epoch 16/1000\n",
      "5703/5703 [==============================] - 0s 36us/step - loss: 3.0449e-04 - val_loss: 2.9721e-04\n",
      "Epoch 17/1000\n",
      "5703/5703 [==============================] - 0s 34us/step - loss: 2.6836e-04 - val_loss: 2.6587e-04\n",
      "Epoch 18/1000\n",
      "5703/5703 [==============================] - 0s 33us/step - loss: 2.4073e-04 - val_loss: 2.4131e-04\n",
      "Epoch 19/1000\n",
      "5703/5703 [==============================] - 0s 37us/step - loss: 2.1922e-04 - val_loss: 2.2080e-04\n",
      "Epoch 20/1000\n",
      "5703/5703 [==============================] - 0s 34us/step - loss: 2.0194e-04 - val_loss: 2.0412e-04\n",
      "Epoch 21/1000\n",
      "5703/5703 [==============================] - 0s 36us/step - loss: 1.8823e-04 - val_loss: 1.9174e-04\n",
      "Epoch 22/1000\n",
      "5703/5703 [==============================] - 0s 34us/step - loss: 1.7653e-04 - val_loss: 1.7969e-04\n",
      "Epoch 23/1000\n",
      "5703/5703 [==============================] - 0s 34us/step - loss: 1.6640e-04 - val_loss: 1.7119e-04\n",
      "Epoch 24/1000\n",
      "5703/5703 [==============================] - 0s 36us/step - loss: 1.5852e-04 - val_loss: 1.6175e-04\n",
      "Epoch 25/1000\n",
      "5703/5703 [==============================] - 0s 35us/step - loss: 1.5172e-04 - val_loss: 1.5495e-04\n",
      "Epoch 26/1000\n",
      "5703/5703 [==============================] - 0s 35us/step - loss: 1.4631e-04 - val_loss: 1.4995e-04\n",
      "Epoch 27/1000\n",
      "5703/5703 [==============================] - 0s 35us/step - loss: 1.4184e-04 - val_loss: 1.4467e-04\n",
      "Epoch 28/1000\n",
      "5703/5703 [==============================] - 0s 35us/step - loss: 1.3761e-04 - val_loss: 1.4111e-04\n",
      "Epoch 29/1000\n",
      "5703/5703 [==============================] - 0s 33us/step - loss: 1.3411e-04 - val_loss: 1.3723e-04\n",
      "Epoch 30/1000\n",
      "5703/5703 [==============================] - 0s 35us/step - loss: 1.3169e-04 - val_loss: 1.3592e-04\n",
      "Epoch 31/1000\n",
      "5703/5703 [==============================] - 0s 34us/step - loss: 1.2896e-04 - val_loss: 1.3239e-04\n",
      "Epoch 32/1000\n",
      "5703/5703 [==============================] - 0s 34us/step - loss: 1.2674e-04 - val_loss: 1.3115e-04\n",
      "Epoch 33/1000\n",
      "5703/5703 [==============================] - 0s 34us/step - loss: 1.2519e-04 - val_loss: 1.2836e-04\n",
      "Epoch 34/1000\n",
      "5703/5703 [==============================] - 0s 34us/step - loss: 1.2269e-04 - val_loss: 1.2704e-04\n",
      "Epoch 35/1000\n",
      "5703/5703 [==============================] - 0s 37us/step - loss: 1.2120e-04 - val_loss: 1.2474e-04\n",
      "Epoch 36/1000\n",
      "5703/5703 [==============================] - 0s 35us/step - loss: 1.1942e-04 - val_loss: 1.2299e-04\n",
      "Epoch 37/1000\n",
      "5703/5703 [==============================] - 0s 34us/step - loss: 1.1806e-04 - val_loss: 1.2275e-04\n",
      "Epoch 38/1000\n",
      "5703/5703 [==============================] - 0s 34us/step - loss: 1.1739e-04 - val_loss: 1.2200e-04\n",
      "Epoch 39/1000\n",
      "5703/5703 [==============================] - 0s 34us/step - loss: 1.1584e-04 - val_loss: 1.1896e-04\n",
      "Epoch 40/1000\n",
      "5703/5703 [==============================] - 0s 34us/step - loss: 1.1454e-04 - val_loss: 1.1847e-04\n",
      "Epoch 41/1000\n",
      "5703/5703 [==============================] - 0s 35us/step - loss: 1.1332e-04 - val_loss: 1.1678e-04\n",
      "Epoch 42/1000\n",
      "5703/5703 [==============================] - 0s 35us/step - loss: 1.1229e-04 - val_loss: 1.1607e-04\n",
      "Epoch 43/1000\n",
      "5703/5703 [==============================] - 0s 33us/step - loss: 1.1121e-04 - val_loss: 1.1507e-04\n",
      "Epoch 44/1000\n",
      "5703/5703 [==============================] - 0s 34us/step - loss: 1.1041e-04 - val_loss: 1.1703e-04\n",
      "Epoch 45/1000\n",
      "5703/5703 [==============================] - 0s 34us/step - loss: 1.0936e-04 - val_loss: 1.1296e-04\n",
      "Epoch 46/1000\n",
      "5703/5703 [==============================] - 0s 34us/step - loss: 1.0838e-04 - val_loss: 1.1319e-04\n",
      "Epoch 47/1000\n",
      "5703/5703 [==============================] - 0s 34us/step - loss: 1.0832e-04 - val_loss: 1.1169e-04\n",
      "Epoch 48/1000\n",
      "5703/5703 [==============================] - 0s 35us/step - loss: 1.0707e-04 - val_loss: 1.1104e-04\n",
      "Epoch 49/1000\n",
      "5703/5703 [==============================] - 0s 34us/step - loss: 1.0673e-04 - val_loss: 1.1029e-04\n",
      "Epoch 50/1000\n",
      "5703/5703 [==============================] - 0s 35us/step - loss: 1.0537e-04 - val_loss: 1.1091e-04\n",
      "Epoch 51/1000\n",
      "5703/5703 [==============================] - 0s 33us/step - loss: 1.0471e-04 - val_loss: 1.0918e-04\n",
      "Epoch 52/1000\n",
      "5703/5703 [==============================] - 0s 35us/step - loss: 1.0415e-04 - val_loss: 1.0855e-04\n",
      "Epoch 53/1000\n",
      "5703/5703 [==============================] - 0s 34us/step - loss: 1.0416e-04 - val_loss: 1.0834e-04\n",
      "Epoch 54/1000\n",
      "5703/5703 [==============================] - 0s 35us/step - loss: 1.0298e-04 - val_loss: 1.0748e-04\n",
      "Epoch 55/1000\n",
      "5703/5703 [==============================] - 0s 34us/step - loss: 1.0251e-04 - val_loss: 1.0953e-04\n",
      "Epoch 56/1000\n",
      "5703/5703 [==============================] - 0s 34us/step - loss: 1.0258e-04 - val_loss: 1.0742e-04\n",
      "Epoch 57/1000\n",
      "5703/5703 [==============================] - 0s 34us/step - loss: 1.0158e-04 - val_loss: 1.0752e-04\n",
      "Epoch 58/1000\n",
      "5703/5703 [==============================] - 0s 33us/step - loss: 1.0072e-04 - val_loss: 1.0560e-04\n",
      "Epoch 59/1000\n",
      "5703/5703 [==============================] - 0s 34us/step - loss: 1.0075e-04 - val_loss: 1.0536e-04\n",
      "Epoch 60/1000\n",
      "5703/5703 [==============================] - 0s 33us/step - loss: 1.0107e-04 - val_loss: 1.1003e-04\n",
      "Epoch 61/1000\n",
      "5703/5703 [==============================] - 0s 34us/step - loss: 1.0121e-04 - val_loss: 1.0557e-04\n",
      "Epoch 62/1000\n",
      "5703/5703 [==============================] - 0s 35us/step - loss: 9.8920e-05 - val_loss: 1.0519e-04\n",
      "Epoch 63/1000\n",
      "5703/5703 [==============================] - 0s 30us/step - loss: 9.9373e-05 - val_loss: 1.0770e-04\n",
      "Epoch 64/1000\n",
      "5703/5703 [==============================] - 0s 32us/step - loss: 9.8860e-05 - val_loss: 1.0356e-04\n",
      "Epoch 65/1000\n",
      "5703/5703 [==============================] - 0s 34us/step - loss: 9.9033e-05 - val_loss: 1.0233e-04\n",
      "Epoch 66/1000\n",
      "5703/5703 [==============================] - 0s 34us/step - loss: 9.7873e-05 - val_loss: 1.0185e-04\n",
      "Epoch 67/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5703/5703 [==============================] - 0s 34us/step - loss: 9.8812e-05 - val_loss: 1.0876e-04\n",
      "Epoch 68/1000\n",
      "5703/5703 [==============================] - 0s 30us/step - loss: 9.8778e-05 - val_loss: 1.0209e-04\n",
      "Epoch 69/1000\n",
      "5703/5703 [==============================] - 0s 28us/step - loss: 9.6744e-05 - val_loss: 1.0359e-04\n",
      "Epoch 70/1000\n",
      "5703/5703 [==============================] - 0s 35us/step - loss: 9.7634e-05 - val_loss: 1.0278e-04\n",
      "Epoch 71/1000\n",
      "5703/5703 [==============================] - 0s 33us/step - loss: 9.7015e-05 - val_loss: 1.0213e-04\n",
      "Epoch 72/1000\n",
      "5703/5703 [==============================] - 0s 34us/step - loss: 9.7440e-05 - val_loss: 1.0278e-04\n",
      "Epoch 73/1000\n",
      "5703/5703 [==============================] - 0s 30us/step - loss: 9.6486e-05 - val_loss: 1.0345e-04\n",
      "Epoch 74/1000\n",
      "5703/5703 [==============================] - 0s 34us/step - loss: 9.6421e-05 - val_loss: 1.0045e-04\n",
      "Epoch 75/1000\n",
      "5703/5703 [==============================] - 0s 32us/step - loss: 9.5885e-05 - val_loss: 1.0044e-04\n",
      "Epoch 76/1000\n",
      "5703/5703 [==============================] - 0s 34us/step - loss: 9.5539e-05 - val_loss: 1.0023e-04\n",
      "Epoch 77/1000\n",
      "5703/5703 [==============================] - 0s 34us/step - loss: 9.5210e-05 - val_loss: 1.0156e-04\n",
      "Epoch 78/1000\n",
      "5703/5703 [==============================] - 0s 34us/step - loss: 9.5429e-05 - val_loss: 9.9611e-05\n",
      "Epoch 79/1000\n",
      "5703/5703 [==============================] - 0s 33us/step - loss: 9.5328e-05 - val_loss: 1.0227e-04\n",
      "Epoch 80/1000\n",
      "5703/5703 [==============================] - 0s 33us/step - loss: 9.5402e-05 - val_loss: 1.0111e-04\n",
      "Epoch 81/1000\n",
      "5703/5703 [==============================] - 0s 33us/step - loss: 9.5035e-05 - val_loss: 9.9389e-05\n",
      "Epoch 82/1000\n",
      "5703/5703 [==============================] - 0s 29us/step - loss: 9.3935e-05 - val_loss: 1.0022e-04\n",
      "Epoch 83/1000\n",
      "5703/5703 [==============================] - 0s 34us/step - loss: 9.4699e-05 - val_loss: 1.0119e-04\n",
      "Epoch 84/1000\n",
      "5703/5703 [==============================] - 0s 32us/step - loss: 9.4489e-05 - val_loss: 9.8925e-05\n",
      "Epoch 85/1000\n",
      "5703/5703 [==============================] - 0s 34us/step - loss: 9.3865e-05 - val_loss: 9.8637e-05\n",
      "Epoch 86/1000\n",
      "5703/5703 [==============================] - 0s 33us/step - loss: 9.4456e-05 - val_loss: 9.9307e-05\n",
      "Epoch 87/1000\n",
      "5703/5703 [==============================] - 0s 33us/step - loss: 9.3651e-05 - val_loss: 9.9167e-05\n",
      "Epoch 88/1000\n",
      "5703/5703 [==============================] - 0s 32us/step - loss: 9.3633e-05 - val_loss: 1.0000e-04\n",
      "Epoch 89/1000\n",
      "5703/5703 [==============================] - 0s 33us/step - loss: 9.3226e-05 - val_loss: 9.8222e-05\n",
      "Epoch 90/1000\n",
      "5703/5703 [==============================] - 0s 33us/step - loss: 9.3687e-05 - val_loss: 1.0058e-04\n",
      "Epoch 91/1000\n",
      "5703/5703 [==============================] - 0s 33us/step - loss: 9.2711e-05 - val_loss: 9.8153e-05\n",
      "Epoch 92/1000\n",
      "5703/5703 [==============================] - 0s 33us/step - loss: 9.3410e-05 - val_loss: 9.8349e-05\n",
      "Epoch 93/1000\n",
      "5703/5703 [==============================] - 0s 33us/step - loss: 9.3089e-05 - val_loss: 1.0023e-04\n",
      "Epoch 94/1000\n",
      "5703/5703 [==============================] - 0s 34us/step - loss: 9.3264e-05 - val_loss: 9.8045e-05\n",
      "Epoch 95/1000\n",
      "5703/5703 [==============================] - 0s 32us/step - loss: 9.2253e-05 - val_loss: 9.8102e-05\n",
      "Epoch 96/1000\n",
      "5703/5703 [==============================] - 0s 33us/step - loss: 9.2288e-05 - val_loss: 9.7693e-05\n",
      "Epoch 97/1000\n",
      "5703/5703 [==============================] - 0s 28us/step - loss: 9.3366e-05 - val_loss: 9.7330e-05\n",
      "Epoch 98/1000\n",
      "5703/5703 [==============================] - 0s 33us/step - loss: 9.2313e-05 - val_loss: 9.9386e-05\n",
      "Epoch 99/1000\n",
      "5703/5703 [==============================] - 0s 34us/step - loss: 9.3003e-05 - val_loss: 9.7441e-05\n",
      "Epoch 100/1000\n",
      "5703/5703 [==============================] - 0s 32us/step - loss: 9.1823e-05 - val_loss: 9.7891e-05\n",
      "Epoch 101/1000\n",
      "5703/5703 [==============================] - 0s 32us/step - loss: 9.2539e-05 - val_loss: 9.9243e-05\n",
      "Epoch 102/1000\n",
      "5703/5703 [==============================] - 0s 34us/step - loss: 9.3211e-05 - val_loss: 9.7074e-05\n",
      "Epoch 103/1000\n",
      "5703/5703 [==============================] - 0s 34us/step - loss: 9.1665e-05 - val_loss: 9.8542e-05\n",
      "Epoch 104/1000\n",
      "5703/5703 [==============================] - 0s 34us/step - loss: 9.1634e-05 - val_loss: 9.6434e-05\n",
      "Epoch 105/1000\n",
      "5703/5703 [==============================] - 0s 33us/step - loss: 9.1198e-05 - val_loss: 9.8443e-05\n",
      "Epoch 106/1000\n",
      "5703/5703 [==============================] - 0s 34us/step - loss: 9.1505e-05 - val_loss: 9.6833e-05\n",
      "Epoch 107/1000\n",
      "5703/5703 [==============================] - 0s 24us/step - loss: 9.1549e-05 - val_loss: 9.6650e-05\n",
      "Epoch 108/1000\n",
      "5703/5703 [==============================] - 0s 23us/step - loss: 9.1469e-05 - val_loss: 9.9498e-05\n",
      "Epoch 109/1000\n",
      "5703/5703 [==============================] - 0s 26us/step - loss: 9.1632e-05 - val_loss: 9.6117e-05\n",
      "Epoch 110/1000\n",
      "5703/5703 [==============================] - 0s 29us/step - loss: 9.1171e-05 - val_loss: 9.7940e-05\n",
      "Epoch 111/1000\n",
      "5703/5703 [==============================] - 0s 30us/step - loss: 9.2691e-05 - val_loss: 9.7274e-05\n",
      "Epoch 112/1000\n",
      "5703/5703 [==============================] - 0s 27us/step - loss: 9.1657e-05 - val_loss: 9.7568e-05\n",
      "Epoch 113/1000\n",
      "5703/5703 [==============================] - 0s 33us/step - loss: 9.0788e-05 - val_loss: 9.4922e-05\n",
      "Epoch 114/1000\n",
      "5703/5703 [==============================] - 0s 29us/step - loss: 9.1260e-05 - val_loss: 9.6676e-05\n",
      "Epoch 115/1000\n",
      "5703/5703 [==============================] - 0s 28us/step - loss: 9.1126e-05 - val_loss: 9.9273e-05\n",
      "Epoch 116/1000\n",
      "5703/5703 [==============================] - 0s 29us/step - loss: 9.0539e-05 - val_loss: 9.6809e-05\n",
      "Epoch 117/1000\n",
      "5703/5703 [==============================] - 0s 32us/step - loss: 9.0026e-05 - val_loss: 9.5592e-05\n",
      "Epoch 118/1000\n",
      "5703/5703 [==============================] - 0s 27us/step - loss: 9.0497e-05 - val_loss: 9.7160e-05\n",
      "Epoch 119/1000\n",
      "5703/5703 [==============================] - 0s 27us/step - loss: 9.1184e-05 - val_loss: 9.5533e-05\n",
      "Epoch 120/1000\n",
      "5703/5703 [==============================] - 0s 26us/step - loss: 9.0792e-05 - val_loss: 9.6672e-05\n",
      "Epoch 121/1000\n",
      "5703/5703 [==============================] - 0s 26us/step - loss: 9.0438e-05 - val_loss: 9.5713e-05\n",
      "Epoch 122/1000\n",
      "5703/5703 [==============================] - 0s 26us/step - loss: 9.0398e-05 - val_loss: 9.5126e-05\n",
      "Epoch 123/1000\n",
      "5703/5703 [==============================] - 0s 26us/step - loss: 8.9891e-05 - val_loss: 9.7624e-05\n",
      "Epoch 124/1000\n",
      "5703/5703 [==============================] - 0s 25us/step - loss: 9.1185e-05 - val_loss: 9.6249e-05\n",
      "Epoch 125/1000\n",
      "5703/5703 [==============================] - 0s 25us/step - loss: 9.0798e-05 - val_loss: 9.7893e-05\n",
      "Epoch 126/1000\n",
      "5703/5703 [==============================] - 0s 26us/step - loss: 8.9421e-05 - val_loss: 1.0002e-04\n",
      "Epoch 127/1000\n",
      "5703/5703 [==============================] - 0s 26us/step - loss: 9.0058e-05 - val_loss: 9.5089e-05\n",
      "Epoch 128/1000\n",
      "5703/5703 [==============================] - 0s 26us/step - loss: 9.0589e-05 - val_loss: 9.6480e-05\n",
      "Epoch 129/1000\n",
      "5703/5703 [==============================] - 0s 27us/step - loss: 9.0752e-05 - val_loss: 9.4635e-05\n",
      "Epoch 130/1000\n",
      "5703/5703 [==============================] - 0s 26us/step - loss: 8.9938e-05 - val_loss: 9.8207e-05\n",
      "Epoch 131/1000\n",
      "5703/5703 [==============================] - 0s 26us/step - loss: 9.0017e-05 - val_loss: 9.7113e-05\n",
      "Epoch 132/1000\n",
      "5703/5703 [==============================] - 0s 26us/step - loss: 8.9318e-05 - val_loss: 9.5281e-05\n",
      "Epoch 133/1000\n",
      "5703/5703 [==============================] - 0s 25us/step - loss: 8.9966e-05 - val_loss: 9.6833e-05\n",
      "Epoch 134/1000\n",
      "5703/5703 [==============================] - 0s 26us/step - loss: 8.9381e-05 - val_loss: 9.5100e-05\n",
      "Epoch 135/1000\n",
      "5703/5703 [==============================] - 0s 26us/step - loss: 9.0491e-05 - val_loss: 9.6113e-05\n",
      "Epoch 136/1000\n",
      "5703/5703 [==============================] - 0s 25us/step - loss: 8.8886e-05 - val_loss: 9.4102e-05\n",
      "Epoch 137/1000\n",
      "5703/5703 [==============================] - 0s 25us/step - loss: 8.8779e-05 - val_loss: 9.6647e-05\n",
      "Epoch 138/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5703/5703 [==============================] - 0s 26us/step - loss: 8.9187e-05 - val_loss: 9.5304e-05\n",
      "Epoch 139/1000\n",
      "5703/5703 [==============================] - 0s 29us/step - loss: 8.9695e-05 - val_loss: 9.4911e-05\n",
      "Epoch 140/1000\n",
      "5703/5703 [==============================] - 0s 30us/step - loss: 9.0247e-05 - val_loss: 9.5618e-05\n",
      "Epoch 141/1000\n",
      "5703/5703 [==============================] - 0s 29us/step - loss: 8.8917e-05 - val_loss: 9.5309e-05\n",
      "Epoch 142/1000\n",
      "5703/5703 [==============================] - 0s 27us/step - loss: 8.9486e-05 - val_loss: 9.9071e-05\n",
      "Epoch 143/1000\n",
      "5703/5703 [==============================] - 0s 29us/step - loss: 8.8627e-05 - val_loss: 9.6157e-05\n",
      "Epoch 144/1000\n",
      "5703/5703 [==============================] - 0s 29us/step - loss: 9.0532e-05 - val_loss: 9.8265e-05\n",
      "Epoch 145/1000\n",
      "5703/5703 [==============================] - 0s 29us/step - loss: 8.9891e-05 - val_loss: 9.4332e-05\n",
      "Epoch 146/1000\n",
      "5703/5703 [==============================] - 0s 29us/step - loss: 8.9280e-05 - val_loss: 9.5706e-05\n",
      "Epoch 147/1000\n",
      "5703/5703 [==============================] - 0s 30us/step - loss: 9.1133e-05 - val_loss: 9.7513e-05\n",
      "Epoch 148/1000\n",
      "5703/5703 [==============================] - 0s 28us/step - loss: 8.8668e-05 - val_loss: 9.6008e-05\n",
      "Epoch 149/1000\n",
      "5703/5703 [==============================] - 0s 27us/step - loss: 9.1659e-05 - val_loss: 9.5361e-05\n",
      "Epoch 150/1000\n",
      "5703/5703 [==============================] - 0s 26us/step - loss: 8.8468e-05 - val_loss: 9.4499e-05\n",
      "Epoch 151/1000\n",
      "5703/5703 [==============================] - 0s 26us/step - loss: 8.9095e-05 - val_loss: 9.5779e-05\n",
      "Epoch 152/1000\n",
      "5703/5703 [==============================] - 0s 28us/step - loss: 8.9181e-05 - val_loss: 9.3780e-05\n",
      "Epoch 153/1000\n",
      "5703/5703 [==============================] - 0s 26us/step - loss: 8.9020e-05 - val_loss: 9.4642e-05\n",
      "Epoch 154/1000\n",
      "5703/5703 [==============================] - 0s 26us/step - loss: 8.9769e-05 - val_loss: 9.5259e-05\n",
      "Epoch 155/1000\n",
      "5703/5703 [==============================] - 0s 26us/step - loss: 8.8256e-05 - val_loss: 9.3856e-05\n",
      "Epoch 156/1000\n",
      "5703/5703 [==============================] - 0s 26us/step - loss: 8.8556e-05 - val_loss: 9.4027e-05\n",
      "Epoch 157/1000\n",
      "5703/5703 [==============================] - 0s 26us/step - loss: 8.8508e-05 - val_loss: 9.6255e-05\n",
      "Epoch 158/1000\n",
      "5703/5703 [==============================] - 0s 26us/step - loss: 8.8864e-05 - val_loss: 9.3937e-05\n",
      "Epoch 159/1000\n",
      "5703/5703 [==============================] - 0s 27us/step - loss: 8.7692e-05 - val_loss: 9.5119e-05\n",
      "Epoch 160/1000\n",
      "5703/5703 [==============================] - 0s 25us/step - loss: 8.8978e-05 - val_loss: 9.5402e-05\n",
      "Epoch 161/1000\n",
      "5703/5703 [==============================] - 0s 26us/step - loss: 8.9240e-05 - val_loss: 9.5184e-05\n",
      "Epoch 162/1000\n",
      "5703/5703 [==============================] - 0s 26us/step - loss: 8.9228e-05 - val_loss: 9.3602e-05\n",
      "Epoch 163/1000\n",
      "5703/5703 [==============================] - 0s 27us/step - loss: 8.9750e-05 - val_loss: 9.8623e-05\n",
      "Epoch 164/1000\n",
      "5703/5703 [==============================] - 0s 26us/step - loss: 9.0423e-05 - val_loss: 9.3749e-05\n",
      "Epoch 165/1000\n",
      "5703/5703 [==============================] - 0s 26us/step - loss: 8.9259e-05 - val_loss: 9.7695e-05\n",
      "Epoch 166/1000\n",
      "5703/5703 [==============================] - 0s 27us/step - loss: 9.0001e-05 - val_loss: 9.4958e-05\n",
      "Epoch 167/1000\n",
      "5703/5703 [==============================] - 0s 26us/step - loss: 8.8647e-05 - val_loss: 9.3491e-05\n",
      "Epoch 168/1000\n",
      "5703/5703 [==============================] - 0s 27us/step - loss: 8.8111e-05 - val_loss: 9.3450e-05\n",
      "Epoch 169/1000\n",
      "5703/5703 [==============================] - 0s 26us/step - loss: 8.8079e-05 - val_loss: 9.3225e-05\n",
      "Epoch 00169: early stopping\n"
     ]
    }
   ],
   "source": [
    "## run n-n model\n",
    "\n",
    "train = readTrain()\n",
    "train_Aug = augFeatures(train)\n",
    "train_norm = normalize(train_Aug)\n",
    "# change the last day and next day \n",
    "X_train, Y_train = buildTrain(train_norm, 5, 5)\n",
    "X_train, Y_train = shuffle(X_train, Y_train)\n",
    "X_train, Y_train, X_val, Y_val = splitData(X_train, Y_train, 0.1)\n",
    "\n",
    "# from 2 dimmension to 3 dimension\n",
    "Y_train = Y_train[:,:,np.newaxis]\n",
    "Y_val = Y_val[:,:,np.newaxis]\n",
    "\n",
    "model = buildManyToManyModel(X_train.shape)\n",
    "callback = EarlyStopping(monitor=\"loss\", patience=10, verbose=1, mode=\"auto\")\n",
    "model.fit(X_train, Y_train, epochs=1000, batch_size=128, validation_data=(X_val, Y_val), callbacks=[callback])\n",
    "\n",
    "pred = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "[1] 李弘毅 — 機器學習 RNN\n",
    "   http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2017/Lecture/RNN.pdf\n",
    "   https://www.youtube.com/watch?v=xCGidAeyS4M\n",
    "\n",
    "[2] Keras關於LSTM的units參數，還是不理解?\n",
    "   https://www.zhihu.com/question/64470274\n",
    "\n",
    "[3] Many to one and many to many LSTM examples in Keras\n",
    "   https://stackoverflow.com/questions/43034960/many-to-one-and-many-to-many-lstm-examples-in-keras\n",
    "\n",
    "[4] Yahoo — SPDR S&P 500 ETF (SPY)\n",
    "   https://finance.yahoo.com/quote/SPY/history?period1=728236800&period2=1523462400&interval=1d&filter=history&frequency=1d      \n",
    "\n",
    "[5] Wiki — 長短期記憶\n",
    "   https://zh.wikipedia.org/wiki/%E9%95%B7%E7%9F%AD%E6%9C%9F%E8%A8%98%E6%86%B6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
