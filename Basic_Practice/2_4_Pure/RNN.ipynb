{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))    \n",
    "    return e_x / e_x.sum(axis=0)\n",
    "    \n",
    "def sigmoid(x):    \n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_cell_forward(xt, a_prev, parameters):    \n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    xt -- your input data at timestep \"t\", numpy array of shape (n_x, m).\n",
    "    a_prev -- Hidden state at timestep \"t-1\", numpy array of shape (n_a, m)\n",
    "    parameters -- python dictionary containing:\n",
    "                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\n",
    "                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\n",
    "                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
    "                        ba --  Bias, numpy array of shape (n_a, 1)\n",
    "                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
    "    Returns:\n",
    "    a_next -- next hidden state, of shape (n_a, m)\n",
    "    yt_pred -- prediction at timestep \"t\", numpy array of shape (n_y, m)\n",
    "    cache -- tuple of values needed for the backward pass, contains (a_next, a_prev, xt, parameters)\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve parameters from \"parameters\"\n",
    "    Wax = parameters[\"Wax\"]\n",
    "    Waa = parameters[\"Waa\"]\n",
    "    Wya = parameters[\"Wya\"]\n",
    "    ba = parameters[\"ba\"]\n",
    "    by = parameters[\"by\"]    \n",
    "    # compute next activation state using the formula given above\n",
    "    a_next = np.tanh(np.matmul(Wax, xt) + np.matmul(Waa, a_prev) + ba)    \n",
    "    # compute output of the current cell using the formula given above\n",
    "    yt_pred = softmax(np.matmul(Wya, a_next) + by) \n",
    "\n",
    "    # store values you need for backward propagation in cache\n",
    "    cache = (a_next, a_prev, xt, parameters)    \n",
    "    return a_next, yt_pred, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-d0b3986f391c>, line 36)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-3-d0b3986f391c>\"\u001b[1;36m, line \u001b[1;32m36\u001b[0m\n\u001b[1;33m    y_pred[:,:,t] =\u001b[0m\n\u001b[1;37m                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def rnn_forward(x, a0, parameters):    \n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    x -- Input data for every time-step, of shape (n_x, m, T_x).\n",
    "    a0 -- Initial hidden state, of shape (n_a, m)\n",
    "    parameters -- python dictionary containing:\n",
    "                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\n",
    "                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\n",
    "                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
    "                        ba --  Bias numpy array of shape (n_a, 1)\n",
    "                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
    "\n",
    "    Returns:\n",
    "    a -- Hidden states for every time-step, numpy array of shape (n_a, m, T_x)\n",
    "    y_pred -- Predictions for every time-step, numpy array of shape (n_y, m, T_x)\n",
    "    caches -- tuple of values needed for the backward pass, contains (list of caches, x)\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize \"caches\" which will contain the list of all caches\n",
    "    caches = []    \n",
    "    # Retrieve dimensions from shapes of x and parameters[\"Wya\"]\n",
    "    n_x, m, T_x = x.shape\n",
    "    n_y, n_a = parameters[\"Wya\"].shape    \n",
    "    # initialize \"a\" and \"y\" with zeros (≈2 lines)\n",
    "    a = np.zeros((n_a, m, T_x))\n",
    "    y_pred = np.zeros((n_y, m, T_x))    \n",
    "    # Initialize a_next (≈1 line)\n",
    "    a_next = a0    \n",
    "    # loop over all time-steps\n",
    "    for t in range(T_x):        \n",
    "        # Update next hidden state, compute the prediction, get the cache \n",
    "        a_next, yt_pred, cache = rnn_cell_forward(x[:,:,t], a_next, parameters)        \n",
    "        # Save the value of the new \"next\" hidden state in a \n",
    "        a[:,:,t] = a_next        \n",
    "        # Save the value of the prediction in y\n",
    "        y_pred[:,:,t] = \n",
    "        # Append \"cache\" to \"caches\" \n",
    "        caches.append(cache)    \n",
    "        \n",
    "    # store values needed for backward propagation in cache\n",
    "    caches = (caches, x)    \n",
    "    return a, y_pred, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
