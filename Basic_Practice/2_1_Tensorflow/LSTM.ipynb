{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def softmax(x):\n",
    "    return tf.nn.softmax(x)\n",
    "\n",
    "def lstm_cell_forward(xt, a_prev, c_prev, parameters):    \n",
    "    \"\"\"\n",
    "    Implement a single forward step of the LSTM-cell as described in Figure (4)\n",
    "\n",
    "    Arguments:\n",
    "    xt -- your input data at timestep \"t\", numpy array of shape (n_x, m).\n",
    "    a_prev -- Hidden state at timestep \"t-1\", numpy array of shape (n_a, m)\n",
    "    c_prev -- Memory state at timestep \"t-1\", numpy array of shape (n_a, m)\n",
    "    parameters -- python dictionary containing:\n",
    "    Wf -- Weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)\n",
    "    bf -- Bias of the forget gate, numpy array of shape (n_a, 1)\n",
    "    Wi -- Weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)\n",
    "    bi -- Bias of the update gate, numpy array of shape (n_a, 1)\n",
    "    Wc -- Weight matrix of the first \"tanh\", numpy array of shape (n_a, n_a + n_x)\n",
    "    bc --  Bias of the first \"tanh\", numpy array of shape (n_a, 1)\n",
    "    Wo -- Weight matrix of the output gate, numpy array of shape (n_a, n_a + n_x)\n",
    "    bo --  Bias of the output gate, numpy array of shape (n_a, 1)Wy -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
    "    by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
    "\n",
    "    Returns:\n",
    "    a_next -- next hidden state, of shape (n_a, m)\n",
    "    c_next -- next memory state, of shape (n_a, m)\n",
    "    yt_pred -- prediction at timestep \"t\", numpy array of shape (n_y, m)\n",
    "    cache -- tuple of values needed for the backward pass, contains (a_next, c_next, a_prev, c_prev, xt, parameters)\n",
    "    \"\"\"\n",
    "\n",
    "    # 获取参数字典中各个参数\n",
    "    Wf = parameters[\"Wf\"]\n",
    "    bf = parameters[\"bf\"]\n",
    "    Wi = parameters[\"Wi\"]\n",
    "    bi = parameters[\"bi\"]\n",
    "    Wc = parameters[\"Wc\"]\n",
    "    bc = parameters[\"bc\"]\n",
    "    Wo = parameters[\"Wo\"]\n",
    "    bo = parameters[\"bo\"]\n",
    "    Wy = parameters[\"Wy\"]\n",
    "    by = parameters[\"by\"]    \n",
    "    # 获取 xt 和 Wy 的维度参数\n",
    "    n_x, m = xt.shape\n",
    "    n_y, n_a = Wy.shape    \n",
    "    # 拼接 a_prev 和 xt\n",
    "    concat = np.zeros((n_a + n_x, m))\n",
    "    concat[: n_a, :] = a_prev\n",
    "    concat[n_a :, :] = xt    \n",
    "    # 计算遗忘门、更新门、记忆细胞候选值、下一时间步的记忆细胞、输出门和下一时间步的隐状态值\n",
    "    ft = sigmoid(np.matmul(Wf, concat) + bf)\n",
    "    it = sigmoid(np.matmul(Wi, concat) + bi)\n",
    "    cct = np.tanh(np.matmul(Wc, concat) + bc)\n",
    "    c_next = ft*c_prev + it*cct\n",
    "    ot = sigmoid(np.matmul(Wo, concat) + bo)\n",
    "    a_next = ot*np.tanh(c_next)    \n",
    "    # 计算 LSTM 的预测输出\n",
    "    yt_pred = softmax(np.matmul(Wy, a_next) + by)    \n",
    "    # 保存各计算结果值\n",
    "    cache = (a_next, c_next, a_prev, c_prev, ft, it, cct, ot, xt, parameters)    \n",
    "    return a_next, c_next, yt_pred, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "xt = np.random.randn(3,10)\n",
    "a_prev = np.random.randn(5,10)\n",
    "c_prev = np.random.randn(5,10)\n",
    "Wf = np.random.randn(5,5+3)\n",
    "bf = np.random.randn(5,1)\n",
    "Wi = np.random.randn(5,5+3)\n",
    "bi = np.random.randn(5,1)\n",
    "Wo = np.random.randn(5,5+3)\n",
    "bo = np.random.randn(5,1)\n",
    "Wc = np.random.randn(5,5+3)\n",
    "bc = np.random.randn(5,1)\n",
    "Wy = np.random.randn(2,5)\n",
    "by = np.random.randn(2,1)\n",
    "\n",
    "parameters = {\"Wf\":Wf, \"Wi\":Wi, \"Wo\":Wo, \"Wc\":Wc, \"Wy\":Wy, \"bf\":bf, \"bi\":bi, \"bo\":bo, \"bc\":bc, \"by\":by}\n",
    "\n",
    "a_next, c_next, yt, cache = lstm_cell_forward(xt, a_prev, c_prev, parameters)\n",
    "\n",
    "print(\"a_next[4] = \", a_next[4])\n",
    "print(\"a_next.shape = \", a_next.shape)\n",
    "print(\"c_next[2] = \", c_next[2])\n",
    "print(\"c_next.shape = \", c_next.shape)\n",
    "print(\"yt[1] = \", yt[1])\n",
    "print(\"yt.shape = \", yt.shape)\n",
    "print(\"cache[1][3] = \", cache[1][3])\n",
    "print(\"len(cache) = \", len(cache))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_cell_forward(xt, a_prev, c_prev, parameters):    \n",
    "    \"\"\"\n",
    "    Implement a single forward step of the LSTM-cell as described in Figure (4)\n",
    "\n",
    "    Arguments:\n",
    "    xt -- your input data at timestep \"t\", numpy array of shape (n_x, m).\n",
    "    a_prev -- Hidden state at timestep \"t-1\", numpy array of shape (n_a, m)\n",
    "    c_prev -- Memory state at timestep \"t-1\", numpy array of shape (n_a, m)\n",
    "    parameters -- python dictionary containing:\n",
    "    Wf -- Weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)\n",
    "    bf -- Bias of the forget gate, numpy array of shape (n_a, 1)\n",
    "    Wi -- Weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)\n",
    "    bi -- Bias of the update gate, numpy array of shape (n_a, 1)\n",
    "    Wc -- Weight matrix of the first \"tanh\", numpy array of shape (n_a, n_a + n_x)\n",
    "    bc --  Bias of the first \"tanh\", numpy array of shape (n_a, 1)\n",
    "    Wo -- Weight matrix of the output gate, numpy array of shape (n_a, n_a + n_x)\n",
    "    bo --  Bias of the output gate, numpy array of shape (n_a, 1)Wy -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
    "    by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
    "\n",
    "    Returns:\n",
    "    a_next -- next hidden state, of shape (n_a, m)\n",
    "    c_next -- next memory state, of shape (n_a, m)\n",
    "    yt_pred -- prediction at timestep \"t\", numpy array of shape (n_y, m)\n",
    "    cache -- tuple of values needed for the backward pass, contains (a_next, c_next, a_prev, c_prev, xt, parameters)\n",
    "    \"\"\"\n",
    "\n",
    "    # 获取参数字典中各个参数\n",
    "    Wf = parameters[\"Wf\"]\n",
    "    bf = parameters[\"bf\"]\n",
    "    Wi = parameters[\"Wi\"]\n",
    "    bi = parameters[\"bi\"]\n",
    "    Wc = parameters[\"Wc\"]\n",
    "    bc = parameters[\"bc\"]\n",
    "    Wo = parameters[\"Wo\"]\n",
    "    bo = parameters[\"bo\"]\n",
    "    Wy = parameters[\"Wy\"]\n",
    "    by = parameters[\"by\"]    \n",
    "    # 获取 xt 和 Wy 的维度参数\n",
    "    n_x, m = xt.shape\n",
    "    n_y, n_a = Wy.shape    \n",
    "    # 拼接 a_prev 和 xt\n",
    "    concat = np.zeros((n_a + n_x, m))\n",
    "    concat[: n_a, :] = a_prev\n",
    "    concat[n_a :, :] = xt    \n",
    "    # 计算遗忘门、更新门、记忆细胞候选值、下一时间步的记忆细胞、输出门和下一时间步的隐状态值\n",
    "    ft = sigmoid(np.matmul(Wf, concat) + bf)\n",
    "    it = sigmoid(np.matmul(Wi, concat) + bi)\n",
    "    cct = np.tanh(np.matmul(Wc, concat) + bc)\n",
    "    c_next = ft*c_prev + it*cct\n",
    "    ot = sigmoid(np.matmul(Wo, concat) + bo)\n",
    "    a_next = ot*np.tanh(c_next)    \n",
    "    # 计算 LSTM 的预测输出\n",
    "    yt_pred = softmax(np.matmul(Wy, a_next) + by)    \n",
    "    # 保存各计算结果值\n",
    "    cache = (a_next, c_next, a_prev, c_prev, ft, it, cct, ot, xt, parameters)    \n",
    "    return a_next, c_next, yt_pred, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_cell_backward(da_next, dc_next, cache):    \n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    da_next -- Gradients of next hidden state, of shape (n_a, m)\n",
    "    dc_next -- Gradients of next cell state, of shape (n_a, m)\n",
    "    cache -- cache storing information from the forward pass\n",
    "\n",
    "    Returns:\n",
    "    gradients -- python dictionary containing:\n",
    "     dxt -- Gradient of input data at time-step t, of shape (n_x, m)\n",
    "     da_prev -- Gradient w.r.t. the previous hidden state, numpy array of shape (n_a, m)\n",
    "     dc_prev -- Gradient w.r.t. the previous memory state, of shape (n_a, m, T_x)\n",
    "     dWf -- Gradient w.r.t. the weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)\n",
    "     dWi -- Gradient w.r.t. the weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)\n",
    "     dWc -- Gradient w.r.t. the weight matrix of the memory gate, numpy array of shape (n_a, n_a + n_x)\n",
    "     dWo -- Gradient w.r.t. the weight matrix of the output gate, numpy array of shape (n_a, n_a + n_x)\n",
    "     dbf -- Gradient w.r.t. biases of the forget gate, of shape (n_a, 1)\n",
    "     dbi -- Gradient w.r.t. biases of the update gate, of shape (n_a, 1)\n",
    "     dbc -- Gradient w.r.t. biases of the memory gate, of shape (n_a, 1)\n",
    "     dbo -- Gradient w.r.t. biases of the output gate, of shape (n_a, 1)\n",
    "    \"\"\"\n",
    "\n",
    "    # 获取缓存值\n",
    "    (a_next, c_next, a_prev, c_prev, ft, it, cct, ot, xt, parameters) = cache    # 获取 xt 和 a_next 的维度大小\n",
    "    n_x, m = xt.shape\n",
    "    n_a, m = a_next.shape    \n",
    "    # 计算各种门的梯度\n",
    "    dot = da_next * np.tanh(c_next) * ot * (1 - ot)\n",
    "    dcct = dc_next * it + ot * (1 - np.tanh(c_next) ** 2) * it * da_next * cct * (1 - np.tanh(cct) ** 2)\n",
    "    dit = dc_next * cct + ot * (1 - np.tanh(c_next) ** 2) * cct * da_next * it * (1 - it)\n",
    "    dft = dc_next * c_prev + ot * (1 - np.tanh(c_next) ** 2) * c_prev * da_next * ft * (1 - ft)    # 计算各参数的梯度 \n",
    "    dWf = np.dot(dft, np.concatenate((a_prev, xt), axis=0).T)\n",
    "    dWi = np.dot(dit, np.concatenate((a_prev, xt), axis=0).T)\n",
    "    dWc = np.dot(dcct, np.concatenate((a_prev, xt), axis=0).T)\n",
    "    dWo = np.dot(dot, np.concatenate((a_prev, xt), axis=0).T)\n",
    "    dbf = np.sum(dft, axis=1, keepdims=True)\n",
    "    dbi = np.sum(dit, axis=1, keepdims=True)\n",
    "    dbc = np.sum(dcct, axis=1, keepdims=True)\n",
    "    dbo = np.sum(dot, axis=1, keepdims=True)\n",
    "\n",
    "    da_prev = np.dot(parameters['Wf'][:,:n_a].T, dft) + np.dot(parameters['Wi'][:,:n_a].T, dit) + np.dot(parameters['Wc'][:,:n_a].T, dcct) + np.dot(parameters['Wo'][:,:n_a].T, dot)\n",
    "    dc_prev = dc_next*ft + ot*(1-np.square(np.tanh(c_next)))*ft*da_next\n",
    "    dxt = np.dot(parameters['Wf'][:,n_a:].T,dft)+np.dot(parameters['Wi'][:,n_a:].T,dit)+np.dot(parameters['Wc'][:,n_a:].T,dcct)+np.dot(parameters['Wo'][:,n_a:].T,dot) \n",
    "\n",
    "    # 将各梯度保存至字典\n",
    "    gradients = {\"dxt\": dxt, \"da_prev\": da_prev, \"dc_prev\": dc_prev, \"dWf\": dWf,\"dbf\": dbf, \"dWi\": dWi,\"dbi\": dbi, \n",
    "                   \"dWc\": dWc,\"dbc\": dbc, \"dWo\": dWo,\"dbo\": dbo}    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_backward(da, caches):    \n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    da -- Gradients w.r.t the hidden states, numpy-array of shape (n_a, m, T_x)\n",
    "    dc -- Gradients w.r.t the memory states, numpy-array of shape (n_a, m, T_x)\n",
    "    caches -- cache storing information from the forward pass (lstm_forward)\n",
    "\n",
    "    Returns:\n",
    "    gradients -- python dictionary containing:\n",
    "           dx -- Gradient of inputs, of shape (n_x, m, T_x)\n",
    "           da0 -- Gradient w.r.t. the previous hidden state, numpy array of shape (n_a, m)\n",
    "           dWf -- Gradient w.r.t. the weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)\n",
    "           dWi -- Gradient w.r.t. the weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)\n",
    "           dWc -- Gradient w.r.t. the weight matrix of the memory gate, numpy array of shape (n_a, n_a + n_x)\n",
    "           dWo -- Gradient w.r.t. the weight matrix of the save gate, numpy array of shape (n_a, n_a + n_x)\n",
    "           dbf -- Gradient w.r.t. biases of the forget gate, of shape (n_a, 1)\n",
    "           dbi -- Gradient w.r.t. biases of the update gate, of shape (n_a, 1)\n",
    "           dbc -- Gradient w.r.t. biases of the memory gate, of shape (n_a, 1)\n",
    "           dbo -- Gradient w.r.t. biases of the save gate, of shape (n_a, 1)\n",
    "    \"\"\"\n",
    "\n",
    "    # 获取第一个缓存值\n",
    "    (caches, x) = caches\n",
    "    (a1, c1, a0, c0, f1, i1, cc1, o1, x1, parameters) = caches[0]    # 获取 da 和 x1 的形状大小\n",
    "    n_a, m, T_x = da.shape\n",
    "    n_x, m = x1.shape    \n",
    "    # 初始化各梯度值\n",
    "    dx = np.zeros((n_x, m, T_x))\n",
    "    da0 = np.zeros((n_a, m))\n",
    "    da_prevt = np.zeros((n_a, m))\n",
    "    dc_prevt = np.zeros((n_a, m))\n",
    "    dWf = np.zeros((n_a, n_a+n_x))\n",
    "    dWi = np.zeros((n_a, n_a+n_x))\n",
    "    dWc = np.zeros((n_a, n_a+n_x))\n",
    "    dWo = np.zeros((n_a, n_a+n_x))\n",
    "    dbf = np.zeros((n_a, 1))\n",
    "    dbi = np.zeros((n_a, 1))\n",
    "    dbc = np.zeros((n_a, 1))\n",
    "    dbo = np.zeros((n_a, 1))    \n",
    "    # 循环各时间步\n",
    "    for t in reversed(range(T_x)):        \n",
    "        # 使用 lstm 单元反向传播计算各梯度值\n",
    "        gradients = lstm_cell_backward(da[:, :, t] + da_prevt, dc_prevt, caches[t])        \n",
    "        # 保存各梯度值\n",
    "        dx[:,:,t] = gradients['dxt']\n",
    "        dWf = dWf + gradients['dWf']\n",
    "        dWi = dWi + gradients['dWi']\n",
    "        dWc = dWc + gradients['dWc']\n",
    "        dWo = dWo + gradients['dWo']\n",
    "        dbf = dbf + gradients['dbf']\n",
    "        dbi = dbi + gradients['dbi']\n",
    "        dbc = dbc + gradients['dbc']\n",
    "        dbo = dbo + gradients['dbo']\n",
    "\n",
    "    da0 = gradients['da_prev']\n",
    "\n",
    "    gradients = {\"dx\": dx, \"da0\": da0, \"dWf\": dWf,\"dbf\": dbf, \"dWi\": dWi,\"dbi\": dbi,                \n",
    "    \"dWc\": dWc,\"dbc\": dbc, \"dWo\": dWo,\"dbo\": dbo}    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 1s 821us/step - loss: 12.1942 - accuracy: 0.1080 - val_loss: 12.5621 - val_accuracy: 0.0800\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 0s 169us/step - loss: 13.7278 - accuracy: 0.1170 - val_loss: 13.0358 - val_accuracy: 0.0900\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 0s 175us/step - loss: 14.0080 - accuracy: 0.1240 - val_loss: 13.1308 - val_accuracy: 0.0900\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 0s 163us/step - loss: 14.0862 - accuracy: 0.1240 - val_loss: 13.1459 - val_accuracy: 0.0900\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 0s 200us/step - loss: 14.0772 - accuracy: 0.1240 - val_loss: 13.1887 - val_accuracy: 0.0900\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x217308da808>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
